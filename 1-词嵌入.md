### 词嵌入

---

**把词映射为实数域向量的技术**也叫词嵌入（word embedding）

近年来，词嵌⼊已逐渐成为⾃然语⾔处理的基础知识

早期是**基于规则**的方法进行转化，而现代的方法是**基于统计机器学习**的方法

#### §1.1 离散表示

##### :one:One-hot编码（独热编码）

特征工程中常用的方式，步骤如下

- [x] 构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1

- [x] 每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示


> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> One-hot表示为：
>
> John: **[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> likes: **[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> ...

**为什么使用独热编码？**

>  独热编码将离散特征的取值扩展到了**欧式空间**，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用独热编码，**会让特征之间的距离计算更加合理**。

**使用独热编码表示文本的缺点？**

> - 随着语料库的增加，数据特征的维度会越来越大，产生一个**维度很高**，又很**稀疏**的矩阵。 
> -  这种表示方法的分词顺序和在句子中的**顺序**是无关的，不能保留词与词之间的**关系信息**。



##### :two:词袋模型(BOW)

像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式**不考虑文法以及词的顺序**

文档的向量表示可以直接将各词的词向量表示**加和**

> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 同样得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> 第一句的向量表示为：**[1,2,1,1,1,0,0,0,1,1]**，其中的2表示**likes**在该句中出现了2次，依次类推

**词袋模型表示文本的缺点？**

> - 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
> - 词与词之间是没有**顺序关系**的。



##### :three:TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是**词频**(Term Frequency)，IDF意思是**逆文本频率指数**(Inverse Document Frequency)

- [x] 字词的重要性随着它在**文件中**出现的次数成正比增加，但同时会随着它在**语料库**中出现的频率成反比下降。
- [x] 一个词语在**一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章**。

计算公式（分母加1是为了防止分母为0）

$$
TF_{w} = \frac{在某一类词条w出现的次数}{此类中所有的词条数目}
$$

$$
IDF = log(\frac{语料库的总文档数}{包含词条w的文档总数+1})
$$

$$
TF-IDF = TF * IDF
$$

**使用TF-IDF的缺点?**

> * 仅以“词频”度量词的重要性，后续构成文档的特征值序列，词之间各自独立，**无法反映序列信息**
> * 易受数据集偏斜的影响，如某一类别的文档偏多，会导致IDF低估
>   * 处理方式：增加类别权重
> * 没有考虑类内、类间分布偏差（被用于特征选择时）
>   * 比如只有2类数据，文档总数200，类1,类2各100个文档
>   * term1只出现在类1的所有100个文档，在类1出现总次数500；term2在类1出现次数也是500，但是类1和类2各有50个文档出现term2
>   * 此时对类1，计算两个term得到的TF-IDF结果是一样的；无法反映term1对类1的重要性



##### :four:n-gram模型

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是**滑窗的大小**

- [x] 例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。
- [x] 该模型**考虑了词的顺序**。

>  例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 可以构造一个词典，{"John likes”: 1, "likes to”: 2, "to watch”: 3, "watch movies”: 4, "Mary likes”: 5, "likes too”: 6, "John also”: 7, "also likes”: 8, “watch football”: 9, "football games": 10}
>
> 那么第一句的向量表示为：**[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]**，其中第一个1表示John likes在该句中出现了1次，依次类推。

**使用n-gram的缺点？**

> 随着n的大小增加，词表会成指数型膨胀，会越来越大



##### :five: 离散表示存在的问题

- 无法衡量词向量之间的关系。
- 词表的维度随着语料库的增长而膨胀。
- n-gram词序列随语料库增长呈指数型膨胀，更加快。
- 离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的

---

#### §1.2 分布式表示

**用一个词附近的其它词来表示该词**，这是现代统计自然语言处理中最有创见的想法之一。当初科学家发明这种方法是基于人的语言表达，认为一个词是由这个词的周边词汇一起来构成精确的语义信息。

##### :one:共现矩阵

词文档的共现矩阵主要用于发现主题(topic)

局域窗中的word-word共现矩阵可以挖掘语法和语义信息

>  例子：I like deep learning. I like NLP. I enjoy flying
>
> 以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}
>
> 可以得到一个共现矩阵(对称矩阵)

| counts       | I    | like | enjoy | deep | learning | NLP  | flying | .    |
| ------------ | ---- | ---- | ----- | ---- | -------- | ---- | ------ | ---- |
| **I**        | 0    | 2    | 1     | 0    | 0        | 0    | 0      | 0    |
| **like**     | 2    | 0    | 0     | 1    | 0        | 1    | 0      | 0    |
| **enjoy**    | 1    | 0    | 0     | 0    | 0        | 0    | 1      | 0    |
| **deep**     | 0    | 1    | 0     | 0    | 1        | 0    | 0      | 0    |
| **learning** | 0    | 0    | 0     | 1    | 0        | 0    | 0      | 1    |
| **NLP**      | 0    | 1    | 0     | 0    | 0        | 0    | 0      | 1    |
| **flying**   | 0    | 0    | 1     | 0    | 0        | 0    | 0      | 1    |
| **.**        | 0    | 0    | 0     | 0    | 1        | 1    | 1      | 0    |

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性

**共现矩阵存在的问题？**

> - 向量维数随着词典大小线性增长。
> - 存储整个词典的空间消耗非常大。
> - 一些模型如文本分类模型会面临稀疏性问题。
> - **模型会欠稳定，每新增一份语料进来，稳定性就会变化。**

---
