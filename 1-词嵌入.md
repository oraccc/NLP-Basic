[TOC]

---

**把词映射为实数域向量的技术**也叫词嵌入（word embedding）

近年来，词嵌⼊已逐渐成为⾃然语⾔处理的基础知识

早期是**基于规则**的方法进行转化，而现代的方法是**基于统计机器学习**的方法



#### §1.1 离散表示

##### :one:One-hot编码（独热编码）

特征工程中常用的方式，步骤如下

- [x] 构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1

- [x] 每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示


> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> One-hot表示为：
>
> John: **[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> likes: **[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> ...

**为什么使用独热编码？**

>  独热编码将离散特征的取值扩展到了**欧式空间**，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用独热编码，**会让特征之间的距离计算更加合理**。

**使用独热编码表示文本的缺点？**

> - 随着语料库的增加，数据特征的维度会越来越大，产生一个**维度很高**，又很**稀疏**的矩阵。 
> -  这种表示方法的分词顺序和在句子中的**顺序**是无关的，不能保留词与词之间的**关系信息**。



##### :two:词袋模型(BOW)

像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式**不考虑文法以及词的顺序**

文档的向量表示可以直接将各词的词向量表示**加和**

> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 同样得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> 第一句的向量表示为：**[1,2,1,1,1,0,0,0,1,1]**，其中的2表示**likes**在该句中出现了2次，依次类推

**词袋模型表示文本的缺点？**

> - 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
> - 词与词之间是没有**顺序关系**的。



##### :three:TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是**词频**(Term Frequency)，IDF意思是**逆文本频率指数**(Inverse Document Frequency)

- [x] 字词的重要性随着它在**文件中**出现的次数成正比增加，但同时会随着它在**语料库**中出现的频率成反比下降。
- [x] 一个词语在**一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章**。

计算公式（分母加1是为了防止分母为0）

$$
TF_{w} = \frac{在某一类词条w出现的次数}{此类中所有的词条数目}
$$

$$
IDF = log(\frac{语料库的总文档数}{包含词条w的文档总数+1})
$$

$$
TF-IDF = TF * IDF
$$

**使用TF-IDF的缺点?**

> * 仅以“词频”度量词的重要性，后续构成文档的特征值序列，词之间各自独立，**无法反映序列信息**
> * 易受数据集偏斜的影响，如某一类别的文档偏多，会导致IDF低估
>   * 处理方式：增加类别权重
> * 没有考虑类内、类间分布偏差（被用于特征选择时）
>   * 比如只有2类数据，文档总数200，类1,类2各100个文档
>   * term1只出现在类1的所有100个文档，在类1出现总次数500；term2在类1出现次数也是500，但是类1和类2各有50个文档出现term2
>   * 此时对类1，计算两个term得到的TF-IDF结果是一样的；无法反映term1对类1的重要性



##### :four:n-gram模型

