{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力模型实现中英文机器翻译"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.569 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import re \n",
    "from utils.datautil import *\n",
    "from utils.seq2seq_model import *\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "steps_per_checkpoint=200 \n",
    "\n",
    "max_train_data_size= 0#(0: no limit)\n",
    "\n",
    "dropout = 0.9 \n",
    "grad_clip = 5.0\n",
    "batch_size = 60\n",
    "\n",
    "num_layers =2\n",
    "learning_rate =0.5\n",
    "lr_decay_factor =0.99\n",
    "\n",
    "###############翻译\n",
    "hidden_size = 100\n",
    "checkpoint_dir= \"fanyichina/checkpoints/\"\n",
    "\n",
    "_buckets =[(20, 20), (40, 40), (50, 50), (60, 60)]\n",
    "def getfanyiInfo():\n",
    "    vocaben, rev_vocaben=datautil.initialize_vocabulary(os.path.join(datautil.data_dir, datautil.vocabulary_fileen))\n",
    "    vocab_sizeen= len(vocaben)\n",
    "    print(\"vocab_size\",vocab_sizeen)\n",
    "    \n",
    "    vocabch, rev_vocabch=datautil.initialize_vocabulary(os.path.join(datautil.data_dir, datautil.vocabulary_filech))\n",
    "    vocab_sizech= len(vocabch)\n",
    "    print(\"vocab_sizech\",vocab_sizech) \n",
    "    \n",
    "    filesfrom,_=datautil.getRawFileList(datautil.data_dir+\"fromids/\")\n",
    "    filesto,_=datautil.getRawFileList(datautil.data_dir+\"toids/\")\n",
    "    source_train_file_path = filesfrom[0]\n",
    "    target_train_file_path= filesto[0]\n",
    "    return vocab_sizeen,vocab_sizech,rev_vocaben,rev_vocabch,source_train_file_path,target_train_file_path\n",
    "################################################################    \n",
    "#source_train_file_path = os.path.join(datautil.data_dir, \"data_source_test.txt\")\n",
    "#target_train_file_path = os.path.join(datautil.data_dir, \"data_target_test.txt\")    \n",
    "    \n",
    "\n",
    "def main():\n",
    "\t\n",
    "    vocab_sizeen,vocab_sizech,rev_vocaben,rev_vocabch,source_train_file_path,target_train_file_path = getfanyiInfo()\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    print (\"checkpoint_dir is {0}\".format(checkpoint_dir))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = createModel(sess,False,vocab_sizeen,vocab_sizech)\n",
    "        print (\"Using bucket sizes:\")\n",
    "        print (_buckets)\n",
    "\n",
    "\n",
    "        source_test_file_path = source_train_file_path\n",
    "        target_test_file_path = target_train_file_path\n",
    "        \n",
    "        print (source_train_file_path)\n",
    "        print (target_train_file_path)\n",
    "        \n",
    "        train_set = readData(source_train_file_path, target_train_file_path,max_train_data_size)\n",
    "        test_set = readData(source_test_file_path, target_test_file_path,max_train_data_size)\n",
    "        \n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "        print( \"bucket sizes = {0}\".format(train_bucket_sizes))\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "    \n",
    "        # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "        # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "        # the size if i-th training bucket, as used later.\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        \n",
    "        while True:\n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            # 开始训练.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,target_weights, bucket_id, False)\n",
    "            step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "            loss += step_loss / steps_per_checkpoint\n",
    "            current_step += 1\n",
    "            \n",
    "            # 保存检查点，测试数据\n",
    "            if current_step % steps_per_checkpoint == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "                    \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),step_time, perplexity))\n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                # Save checkpoint and zero timer and loss.\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, \"seq2seqtest.ckpt\")\n",
    "                print(checkpoint_path)\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                # Run evals on development set and print their perplexity.\n",
    "                for bucket_id in xrange(len(_buckets)):\n",
    "                    if len(test_set[bucket_id]) == 0:\n",
    "                        print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "                        continue\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(test_set, bucket_id)\n",
    "\n",
    "                    _, eval_loss,output_logits = model.step(sess, encoder_inputs, decoder_inputs,target_weights, bucket_id, True)\n",
    "                    eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "                    print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "                    \n",
    "                    \n",
    "                    inputstr = datautil.ids2texts(reversed([en[0] for en in encoder_inputs]) ,rev_vocaben)\n",
    "                    print(\"输入\",inputstr)\n",
    "                    print(\"输出\",datautil.ids2texts([en[0] for en in decoder_inputs] ,rev_vocabch))\n",
    "  \n",
    "                    outputs = [np.argmax(logit, axis=1)[0] for logit in output_logits]                    \n",
    "                    #outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "                    #print(\"outputs\",outputs,datautil.EOS_ID)\n",
    "                    if datautil.EOS_ID in outputs:\n",
    "                        outputs = outputs[:outputs.index(datautil.EOS_ID)]\n",
    "                        print(\"结果\",datautil.ids2texts(outputs,rev_vocabch))\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "def createModel(session, forward_only,from_vocab_size,to_vocab_size):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "      from_vocab_size,#from\n",
    "      to_vocab_size,#to\n",
    "      _buckets,\n",
    "      hidden_size,\n",
    "      num_layers,\n",
    "      dropout,\n",
    "      grad_clip,\n",
    "      batch_size,\n",
    "      learning_rate,\n",
    "      lr_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=tf.float32)\n",
    "      \n",
    "    print(\"model is ok\")\n",
    "\n",
    "    \n",
    "    ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if ckpt!=None:\n",
    "        model.saver.restore(session, ckpt)\n",
    "        print (\"Reading model parameters from {0}\".format(ckpt))\n",
    "    else:\n",
    "        print (\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())  \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readData(source_path, target_path, max_size=None):\n",
    "\t'''\n",
    "\tThis method directly from tensorflow translation example\n",
    "\t'''\n",
    "\tdata_set = [[] for _ in _buckets]\n",
    "\twith tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "\t\twith tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "\t\t\tsource, target = source_file.readline(), target_file.readline()\n",
    "\t\t\tcounter = 0\n",
    "\t\t\twhile source and target and (not max_size or counter < max_size):\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif counter % 100000 == 0:\n",
    "\t\t\t\t\tprint(\"  reading data line %d\" % counter)\n",
    "\t\t\t\t\tsys.stdout.flush()\n",
    "\t\t\t\tsource_ids = [int(x) for x in source.split()]\n",
    "\t\t\t\ttarget_ids = [int(x) for x in target.split()]\n",
    "\t\t\t\ttarget_ids.append(datautil.EOS_ID)\n",
    "\t\t\t\tfor bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "\t\t\t\t\tif len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "\t\t\t\t\t\tdata_set[bucket_id].append([source_ids, target_ids])\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tsource, target = source_file.readline(), target_file.readline()\n",
    "\treturn data_set\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files, one is ../../data/attention-data/translation/corpus/from/english1w.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "623ed0ec00d341bb244609ee0f891229bed72b94802ac9fdbf49b1ebdc1e4a9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
