{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalization部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(inputs, epsilon = 1e-8, scope=\"ln\"):\n",
    "    \"\"\"\n",
    "    使用层归一layer normalization\n",
    "    tensorflow 在实现 Batch Normalization（各个网络层输出的归一化）时，主要用到nn.moments和batch_normalization\n",
    "    其中moments作用是统计矩，mean 是一阶矩，variance 则是二阶中心矩\n",
    "    tf.nn.moments 计算返回的 mean 和 variance 作为 tf.nn.batch_normalization 参数进一步调用\n",
    "    :param inputs: 一个有2个或更多维度的张量，第一个维度是batch_size\n",
    "    :param epsilon: 很小的数值，防止区域划分错误\n",
    "    :param scope:\n",
    "    :return: 返回一个与inputs相同shape和数据的dtype\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask部分\n",
    " Padding Mask 与 Sequence Mask\n",
    "\n",
    "`tf.reduce_sum()`: 计算张量tensor沿着某一维度的和，可以在求和后降维\n",
    "\n",
    "`tf.sign()`: 返回符号(-1, 0, 1), y= sign(x) = -1 if x<0; 0 if x==0; 1 if x>0.\n",
    "\n",
    "`tf.expand_dims()`: 使用axis参数。给定一个张量输入，这个操作在输入形状的维数索引轴上插入一个维数为1的维度\n",
    "\n",
    "`tf.tile()`: 平铺之意，用于在同一维度上的复制，multiples参数维度与input维度应一致\n",
    "\n",
    "`tf.where(input, a, b)`: a，b均为尺寸一致的tensor，实现a中对应input中true的位置的元素值不变，其余元素由b中对应位置元素替换\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(inputs, queries=None, keys=None, type=None):\n",
    "    \"\"\"\n",
    "    对Keys或Queries进行遮盖\n",
    "    :param inputs: (N, T_q, T_k)\n",
    "    :param queries: (N, T_q, d)\n",
    "    :param keys: (N, T_k, d)\n",
    "    :return:\n",
    "\n",
    "    e.g.,\n",
    "    >> queries = tf.constant([[[1.],\n",
    "                               [2.],\n",
    "                               [0.]]], tf.float32) # (1, 3, 1)\n",
    "    >> keys = tf.constant([[[4.],\n",
    "                            [0.]]], tf.float32)  # (1, 2, 1)\n",
    "    >> inputs = tf.constant([[[4., 0.],\n",
    "                              [8., 0.],\n",
    "                              [0., 0.]]], tf.float32)\n",
    "    >> mask(inputs, queries, keys, \"key\")\n",
    "    array([[[ 4.0000000e+00, -4.2949673e+09],\n",
    "            [ 8.0000000e+00, -4.2949673e+09],\n",
    "            [ 0.0000000e+00, -4.2949673e+09]]], dtype=float32)\n",
    "    \n",
    "    >> inputs = tf.constant([[[1., 0.],\n",
    "                              [1., 0.],\n",
    "                              [1., 0.]]], tf.float32)\n",
    "    >> mask(inputs, queries, keys, \"query\")\n",
    "    array([[[1., 0.],\n",
    "            [1., 0.],\n",
    "            [0., 0.]]], dtype=float32)\n",
    "    \"\"\"\n",
    "\n",
    "    padding_num = -2 ** 32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        # Generate masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # (N, T_k)\n",
    "        masks = tf.expand_dims(masks, 1) # (N, 1, T_k)\n",
    "        masks = tf.tile(masks, [1, tf.shape(queries)[1], 1])  # (N, T_q, T_k)\n",
    "\n",
    "        # Apply masks to inputs\n",
    "        paddings = tf.ones_like(inputs) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)  # (N, T_q, T_k)\n",
    "    elif type in (\"q\", \"query\", \"queries\"):\n",
    "        # Generate masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n",
    "        masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n",
    "        masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n",
    "\n",
    "        # Apply masks to inputs\n",
    "        outputs = inputs*masks\n",
    "    elif type in (\"f\", \"future\", \"right\"):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)\n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "\n",
    "\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
