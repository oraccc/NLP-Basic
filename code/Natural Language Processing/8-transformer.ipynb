{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Normalization部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(inputs, epsilon = 1e-8, scope=\"ln\"):\n",
    "    \"\"\"\n",
    "    使用层归一layer normalization\n",
    "    tensorflow 在实现 Batch Normalization（各个网络层输出的归一化）时，主要用到nn.moments和batch_normalization\n",
    "    其中moments作用是统计矩，mean 是一阶矩，variance 则是二阶中心矩\n",
    "    tf.nn.moments 计算返回的 mean 和 variance 作为 tf.nn.batch_normalization 参数进一步调用\n",
    "    :param inputs: 一个有2个或更多维度的张量，第一个维度是batch_size\n",
    "    :param epsilon: 很小的数值，防止区域划分错误\n",
    "    :param scope:\n",
    "    :return: 返回一个与inputs相同shape和数据的dtype\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask部分\n",
    " Padding Mask 与 Sequence Mask\n",
    "\n",
    "`tf.reduce_sum()`: 计算张量tensor沿着某一维度的和，可以在求和后降维\n",
    "\n",
    "`tf.sign()`: 返回符号(-1, 0, 1), y= sign(x) = -1 if x<0; 0 if x==0; 1 if x>0.\n",
    "\n",
    "`tf.expand_dims()`: 使用axis参数。给定一个张量输入，这个操作在输入形状的维数索引轴上插入一个维数为1的维度\n",
    "\n",
    "`tf.tile()`: 平铺之意，用于在同一维度上的复制，multiples参数维度与input维度应一致\n",
    "\n",
    "`tf.where(input, a, b)`: a，b均为尺寸一致的tensor，实现a中对应input中true的位置的元素值不变，其余元素由b中对应位置元素替换\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(inputs, queries=None, keys=None, type=None):\n",
    "    \"\"\"\n",
    "    对Keys或Queries进行遮盖\n",
    "    :param inputs: (N, T_q, T_k)\n",
    "    :param queries: (N, T_q, d)\n",
    "    :param keys: (N, T_k, d)\n",
    "    :return:\n",
    "\n",
    "    e.g.,\n",
    "    >> queries = tf.constant([[[1.],\n",
    "                               [2.],\n",
    "                               [0.]]], tf.float32) # (1, 3, 1)\n",
    "    >> keys = tf.constant([[[4.],\n",
    "                            [0.]]], tf.float32)  # (1, 2, 1)\n",
    "    >> inputs = tf.constant([[[4., 0.],\n",
    "                              [8., 0.],\n",
    "                              [0., 0.]]], tf.float32)\n",
    "    >> mask(inputs, queries, keys, \"key\")\n",
    "    array([[[ 4.0000000e+00, -4.2949673e+09],\n",
    "            [ 8.0000000e+00, -4.2949673e+09],\n",
    "            [ 0.0000000e+00, -4.2949673e+09]]], dtype=float32)\n",
    "    \n",
    "    >> inputs = tf.constant([[[1., 0.],\n",
    "                              [1., 0.],\n",
    "                              [1., 0.]]], tf.float32)\n",
    "    >> mask(inputs, queries, keys, \"query\")\n",
    "    array([[[1., 0.],\n",
    "            [1., 0.],\n",
    "            [0., 0.]]], dtype=float32)\n",
    "    \"\"\"\n",
    "\n",
    "    padding_num = -2 ** 32 + 1\n",
    "    if type in (\"k\", \"key\", \"keys\"):\n",
    "        # Generate masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # (N, T_k)\n",
    "        masks = tf.expand_dims(masks, 1) # (N, 1, T_k)\n",
    "        masks = tf.tile(masks, [1, tf.shape(queries)[1], 1])  # (N, T_q, T_k)\n",
    "\n",
    "        # Apply masks to inputs\n",
    "        paddings = tf.ones_like(inputs) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)  # (N, T_q, T_k)\n",
    "    elif type in (\"q\", \"query\", \"queries\"):\n",
    "        # Generate masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)\n",
    "        masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n",
    "        masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)\n",
    "\n",
    "        # Apply masks to inputs\n",
    "        outputs = inputs*masks\n",
    "    elif type in (\"f\", \"future\", \"right\"):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, inputs)\n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder Attention\n",
    "此处是两个不同序列之间的attention，与来源于自身的 self-attention 相区别。context-attention有很多，这里使用的是scaled dot-product。通过 query 和 key 的相似性程度来确定 value 的权重分布。\n",
    "\n",
    "实际上这部分代码就是self attention用到的QKV的公式的核心代码，不管是Encoder-Decoder Attention还是Self Attention都是用的这里的scaled dot-product方法。\n",
    "\n",
    "`tf.transpose(X, perm=None)`: 按照新维度序列转置矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V,\n",
    "                                 causality=False, dropout_rate=0.,\n",
    "                                 training=True,\n",
    "                                 scope=\"scaled_dot_product_attention\"):\n",
    "\n",
    "    '''\n",
    "    查看原论文中3.2.1attention计算公式：Attention(Q,K,V)=softmax(Q K^T /√dk ) V\n",
    "    :param Q: 查询，三维张量，[N, T_q, d_k].\n",
    "    :param K: keys值，三维张量，[N, T_k, d_v].\n",
    "    :param V: values值，三维张量，[N, T_k, d_v].\n",
    "    :param causality: 布尔值，如果为True，就会对未来的数值进行遮盖\n",
    "    :param dropout_rate: 0到1之间的一个数值\n",
    "    :param training: 布尔值，用来控制dropout\n",
    "    :param scope:\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "        # dot product\n",
    "        outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "\n",
    "        # scale\n",
    "        outputs /= d_k ** 0.5\n",
    "\n",
    "        # key masking\n",
    "        outputs = mask(outputs, Q, K, type=\"key\")\n",
    "\n",
    "        # causality or future blinding masking\n",
    "        if causality:\n",
    "            outputs = mask(outputs, type=\"future\")\n",
    "\n",
    "        # softmax\n",
    "        outputs = tf.nn.softmax(outputs)\n",
    "        attention = tf.transpose(outputs, [0, 2, 1])\n",
    "        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n",
    "\n",
    "        # query masking\n",
    "        outputs = mask(outputs, Q, K, type=\"query\")\n",
    "\n",
    "        # dropout\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n",
    "\n",
    "        # weighted sum (context vectors)\n",
    "        outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, keys, values,\n",
    "                        num_heads=8,\n",
    "                        dropout_rate=0,\n",
    "                        training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\"):\n",
    "    '''\n",
    "    查看原论文中3.2.2中multihead_attention构建，\n",
    "    这里是将不同的Queries、Keys和values方式线性地投影h次是有益的。\n",
    "    线性投影分别为dk，dk和dv尺寸。在每个预计版本进行queries、keys、values，\n",
    "    然后并行执行attention功能，产生dv维输出值。这些被连接并再次投影，产生最终值\n",
    "    :param queries: 三维张量[N, T_q, d_model]\n",
    "    :param keys: 三维张量[N, T_k, d_model]\n",
    "    :param values: 三维张量[N, T_k, d_model]\n",
    "    :param num_heads: heads数\n",
    "    :param dropout_rate:\n",
    "    :param training: 控制dropout机制\n",
    "    :param causality: 控制是否遮盖\n",
    "    :param scope:\n",
    "    :return: 三维张量(N, T_q, C)\n",
    "    '''\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, d_model, use_bias=False) # (N, T_q, d_model)\n",
    "        K = tf.layers.dense(keys, d_model, use_bias=False) # (N, T_k, d_model)\n",
    "        V = tf.layers.dense(values, d_model, use_bias=False) # (N, T_k, d_model)\n",
    "\n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "\n",
    "        # Attention\n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_, causality, dropout_rate, training)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "\n",
    "        # Normalize\n",
    "        outputs = ln(outputs)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Embedding\n",
    "就目前而言，Transformer 架构还没有提取序列顺序的信息，这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：所有词语都对了，但是无法组成有意义的语句。因此模型对序列中的词语出现的位置进行编码。论文中使用的方法是在偶数位置使用正弦编码，在奇数位置使用余弦编码。\n",
    "\n",
    "`tf.nn.embedding_lookup(params, ids)`: params可以是张量也可以是数组等，id就是对应的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs,\n",
    "                        maxlen,\n",
    "                        masking=True,\n",
    "                        scope=\"positional_encoding\"):\n",
    "\n",
    "    '''\n",
    "    由于模型没有循环和卷积，为了让模型知道句子的编号，\n",
    "    就必须加入某些绝对位置信息，来表示token之间的关系。\n",
    "    positional encoding和embedding有相同的维度，这两个能够相加。\n",
    "    :param inputs:\n",
    "    :param maxlen:\n",
    "    :param masking:\n",
    "    :param scope:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    E = inputs.get_shape().as_list()[-1] # static\n",
    "    N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # position indices\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "            for pos in range(maxlen)])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n",
    "\n",
    "        # lookup\n",
    "        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n",
    "\n",
    "        # masks\n",
    "        if masking:\n",
    "            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n",
    "\n",
    "        return tf.to_float(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整合Encoder与Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(self, xs, training=True):\n",
    "    '''\n",
    "    Returns\n",
    "    memory: encoder outputs. (N, T1, d_model)\n",
    "    '''\n",
    "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "        x, seqlens, sents1 = xs\n",
    "\n",
    "        # embedding\n",
    "        enc = tf.nn.embedding_lookup(self.embeddings, x) # (N, T1, d_model)\n",
    "        enc *= self.hp.d_model**0.5 # scale\n",
    "\n",
    "        enc += positional_encoding(enc, self.hp.maxlen1)\n",
    "        enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n",
    "\n",
    "        ## Blocks\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                # self-attention\n",
    "                enc = multihead_attention(queries=enc,\n",
    "                                            keys=enc,\n",
    "                                            values=enc,\n",
    "                                            num_heads=self.hp.num_heads,\n",
    "                                            dropout_rate=self.hp.dropout_rate,\n",
    "                                            training=training,\n",
    "                                            causality=False)\n",
    "                # feed forward\n",
    "                enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "    memory = enc\n",
    "    return memory, sents1\n",
    "\n",
    "def decode(self, ys, memory, training=True):\n",
    "    '''\n",
    "    memory: encoder outputs. (N, T1, d_model)\n",
    "\n",
    "    Returns\n",
    "    logits: (N, T2, V). float32.\n",
    "    y_hat: (N, T2). int32\n",
    "    y: (N, T2). int32\n",
    "    sents2: (N,). string.\n",
    "    '''\n",
    "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        decoder_inputs, y, seqlens, sents2 = ys\n",
    "\n",
    "        # embedding\n",
    "        dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  # (N, T2, d_model)\n",
    "        dec *= self.hp.d_model ** 0.5  # scale\n",
    "\n",
    "        dec += positional_encoding(dec, self.hp.maxlen2)\n",
    "        dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n",
    "\n",
    "        # Blocks\n",
    "        for i in range(self.hp.num_blocks):\n",
    "            with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                # Masked self-attention (Note that causality is True at this time)\n",
    "                dec = multihead_attention(queries=dec,\n",
    "                                            keys=dec,\n",
    "                                            values=dec,\n",
    "                                            num_heads=self.hp.num_heads,\n",
    "                                            dropout_rate=self.hp.dropout_rate,\n",
    "                                            training=training,\n",
    "                                            causality=True,\n",
    "                                            scope=\"self_attention\")\n",
    "\n",
    "                # Vanilla attention\n",
    "                dec = multihead_attention(queries=dec,\n",
    "                                            keys=memory,\n",
    "                                            values=memory,\n",
    "                                            num_heads=self.hp.num_heads,\n",
    "                                            dropout_rate=self.hp.dropout_rate,\n",
    "                                            training=training,\n",
    "                                            causality=False,\n",
    "                                            scope=\"vanilla_attention\")\n",
    "                ### Feed Forward\n",
    "                dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "\n",
    "    # Final linear projection (embedding weights are shared)\n",
    "    weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n",
    "    logits = tf.einsum('ntd,dk->ntk', dec, weights) # (N, T2, vocab_size)\n",
    "    y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "\n",
    "    return logits, y_hat, y, sents2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
