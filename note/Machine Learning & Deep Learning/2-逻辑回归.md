## 逻辑回归

- [逻辑回归](#逻辑回归)
  - [§2.1 基本概念](#21-基本概念)
    - [Sigmoid](#sigmoid)
    - [损失函数](#损失函数)
  - [§2.2 细节问题](#22-细节问题)


### §2.1 基本概念

> :memo: [Jupyter Notebook 代码](https://github.com/oraccc/NLP-Basic/blob/master/code/Machine%20Learning%20%26%20Deep%20Learning/2-logistic-regression.ipynb)

逻辑回归也称作**logistic回归分析**，是一种广义的线性回归分析模型，属于机器学习中的**监督学习**。

- 其推导过程与计算方式类似于回归的过程，但实际上主要是**用来解决二分类问题**（也可以解决多分类问题）。
- 通过给定的n组数据（训练集）来训练模型，并在训练结束后对给定的一组或多组数据（测试集）进行分类。
  

如果数据是有两个指标，可以用**平面**的点来表示数据，其中一个指标为x轴，另一个为y轴；如果数据有三个指标，可以用**空间**中的点表示数据；如果是p维的话(p>3)，就是**p维空间**中的点。

> 从本质上来说，逻辑回归训练后的模型是平面的**一条直线**（p=2),或是**平面**（p=3)，**超平面**（p>3)。并且这条线或平面把空间中的散点分成两半，属于同一类的数据大多数分布在曲线或平面的同一侧。



#### Sigmoid

把 $Y$ 的结果带入一个非线性变换的**Sigmoid函数**中，即可得到[0,1]之间取值范围的数 $S$，$S$可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么 $S$ 大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/logistics%20regression/sigmoid.jpg" width="400" />

对于二分类，我们便让其中一类标签为0，另一类为1。我们需要一个函数，对于输入的每一组数据 $x^{(i)}$，都能映射成0~1之间的数。并且如果函数值大于0.5，就判定属于1，否则属于0。而且函数中需要待定参数，通过利用样本训练，使得这个参数能够对训练集中的数据有很准确的预测。
$$
h(x^i) = \frac{1}{1+e^{-(W^Tx^i+b)}}
$$

#### 损失函数

使用对数似然函数作为损失函数
$$
Cost(h(x^i),y) = \begin{cases}
-log(h(x^i)) & y=1 \\
-log(1-h(x^i)) & y=0
\end{cases}
$$
公式中的 $y=1$ 表示的是真实值为1时用第一个公式，真实 $y=0$ 用第二个公式计算损失。

为什么要加上log函数呢？可以试想一下，当真实样本为1是，但 $h=0$ 概率，那么 $log0=∞$，这就对模型最大的惩罚力度；当 $h=1$ 时，那么 $log1=0$，相当于没有惩罚，也就是没有损失，达到最优结果。

---



### §2.2 细节问题

**:question:可以进行多分类吗？**

> 可以，我们可以从二分类问题**过渡到多分类问题**(one vs rest)，思路步骤如下：
> - [x] 将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。
> - [x] 然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。
> - [x] 以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率pi，最后我们**取pi中最大的那个概率对应的样本标记**类型作为我们的待预测样本类型。
> 或者可以修改logistic回归，使其变为softmax回归（使用交叉熵作为损失函数）



**:question:逻辑回归常用的优化方法？**

> 一阶（求导）方法：梯度下降，随机梯度下降
>
> 二阶（求导）方法：牛顿法、拟牛顿法
>
> 牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。
>
> 缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。
>
> 拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。



**:question:逻辑回归为什么要对特征进行离散化？**

> - 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了**非线性**，能够提升模型表达能力，加大拟合； 
>   - 离散特征的增加和减少都很容易，易于模型的快速迭代；
> - 稀疏向量内积乘法运算**速度快**，计算结果方便存储，容易扩展；
> - 离散化后的特征对异常数据有很强的**鲁棒性**
>   - 比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰
> - **方便交叉与特征组合**：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
> - **稳定性**：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
> - **简化模型**：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

---

