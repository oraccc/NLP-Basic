## 线性回归

- [线性回归](#线性回归)
  - [§1.1 基本概念](#11-基本概念)
    - [一般表达式与损失函数](#一般表达式与损失函数)
  - [§1.2 各类正则化](#12-各类正则化)
    - [L2正则化（岭回归）](#l2正则化岭回归)
    - [L1正则化 （Lasso回归）](#l1正则化-lasso回归)
    - [ElasticNet](#elasticnet)

### §1.1 基本概念

> :memo: [Jupyter Notebook 代码](https://github.com/oraccc/NLP-Basic/blob/master/code/Machine%20Learning%20%26%20Deep%20Learning/1-linear-regression.ipynb)

- 线性：两个变量之间的关系**是**一次函数关系的——图象**是直线**，叫做线性。
- 非线性：两个变量之间的关系**不是**一次函数关系的——图象**不是直线**，叫做非线性。
- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。



对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。

> 例如：对房价的预测、判断信用评价、电影票房预估等。



#### 一般表达式与损失函数

将向量 $x \in \mathbb{R}^n$ 作为输入，预测标量 $y \in \mathbb{R}$ 作为输出，其中 $\hat{y}$ 为预测的结果
$$
\hat{y} = w^Tx+b
$$
直接给出矩阵的形式
$$
y=Xw+b
$$


$W$ 为系数，$b$ 为偏置项

Loss Function : **MSE**
$$
J=\frac{1}{2m}\sum_{m}^{i=1} (\hat{y}-y)^2
$$
$m$ 为样本的数量

利用**梯度下降法**找到最小值点，也就是最小误差，最后把 $w$ 和 $b$ 给求出来。



### §1.2 各类正则化

#### L2正则化（岭回归）

在标准线性回归中
$$
w=(X^TX)^{-1}X^Ty
$$
其中 $X$ 是一个 $m*n$ 的矩阵，若 $X^TX$ 不是满秩矩阵（$n>m$，行数小于列数），则该矩阵不可逆

为了解决这一个问题，引入岭回归的概念
$$
w=(X^TX+\lambda I)^{-1}X^Ty
$$
其中 $\lambda$ 为岭系数，$I$ 为单位矩阵，则括号内的式子为满秩的可逆矩阵

L2正则化的表达式如下
$$
J=J_0 + \lambda \sum_{w} w^2
$$
$J_0$ 为原先的损失函数

#### L1正则化 （Lasso回归）

$$
J = J_0 + \lambda \sum_{w} |w|
$$

第一项 $J_0$ 表示为原目标，第二项 $\sum_{w}|w|$ 表示L1正则化项，$\lambda$ 表示正则化系数；

机器学习的任务就是通过一些方法（如梯度下降）求出目标函数的最小值；当在原目标函数 $J_0$ 后添加L1正则化项时，就相当于对 $J_0$ 做了一个约束；

令 $L=\lambda \sum_{w}|w|$，则 $J=J_0+L$，此时任务变成**在 $L$ 约束条件下求出 $J_0$ 的最小值的解**；

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/linear%20regression/L1.png" width="250" >

其中，在 $w_1,w_2$ 的二维平面上作图，彩色等值线表示 $J_0$ 的取值，中心代表 $J_0$ 的最小值，黑色方框表示 $L$ 的图形；

图中 $J_0$ 等值线与 $L$ 首次相交的地方就是最优解。即图像的 $(w1,w2)=(0,w)$；

可以想象，因为 $L$ 函数有很多『突出的角』（二维情况下四个，多维情况下更多），$J_0$ 与这些角接触的几率远大于与 $L$ 其他部位接触的几率，而在这些角上，会有很多的权值等于0，这就是为什么正则化可以产生稀疏模型，进而用于特征选择；因为**含有L1正则化项的目标函数的最优解中，有很多参数的值都为0，即得到稀疏模型；**



同理，$w1,w2$ 二维平面下 $L_2$ 正则化项函数图形是一个圆，与方形相比，被磨去了棱角。在取得最优解的时候，模型中参数为0的几率小了很多，即得到的**模型不容易是稀疏模型**；

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/linear%20regression/L2.png" width="250" >

#### ElasticNet

同时结合两者的的思想
$$
min_{w} \frac{1}{2m}[\sum_{i=1}^{m} (\hat{y}-y)^2 + \lambda_1 \sum_{j=1}^n|w| + \lambda_2 \sum_{j=1}^n|w|^2]
$$
在我们发现用Lasso回归太过(**太多特征被稀疏为0**),而岭回归也正则化的不够(**回归系数衰减太慢**)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。



**:question:线性回归为何要求因变量服从正态分布？**

> 线性回归的假设前提是特征与预测值呈线性关系，误差项符合高斯-马尔科夫条件（零均值，零方差，不相关），这时候线性回归是无偏估计。
>
> 噪声符合正态分布，那么因变量也符合分布。在进行线性回归之前，要求因变量近似符合正态分布，否则线性回归效果不佳（有偏估计）