## Seq2Seq



### §6.1 Seq2Seq简介

在自然语⾔处理的很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是⼀段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，

> 例如：
>
> 英语输入：“They”、“are”、“watching”、“.”
>
> 法语输出：“Ils”、“regardent”、“.”

当输入和输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）或者序列到序列模型（**seq2seq模型**）。这两个模型本质上都用到了两个循环神经网络，分别叫做**编码器**和**解码器**。编码器用来分析输入序列，解码器用来生成输出序列。两个循环神经网络是**共同训练**的。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/Seq2Seq/translate.png" width="700" />

上图描述了使用编码器—解码器将上述英语句子翻译成法语句子的⼀种方法。

- 在训练数据集中，我们可以在每个句子后附上特殊符号“\<eos>”（end of sequence）以表示序列的终止。
- 编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“\<eos>”。上图中使用了编码器**在最终时间步的隐藏状态**作为输⼊句子的表征或编码信息。
- 解码器**在各个时间步中**使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。
- 我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号“\<eos>”。
- 需要注意的是，解码器在最初时间步的输入用到了⼀个表示序列开始的特殊符号“\<bos>”（beginning of sequence）。

---



### §6.2 编码器与解码器

#### :one:编码器

编码器的作用是把⼀个不定长的输入序列**变换成⼀个定长的背景变量** $c$，并在该背景变量中编码输入序列信息。

常用的编码器是**循环神经网络**。

- 让我们考虑批量大小为 $1$ 的时序数据样本。假设输入序列是 $x_1,...,x_T$，例如 $x_i$ 是输入句子中的第 $i$ 个词。

- 在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $x_t$ 和上个时间步的隐藏状态 $h_{t-1}$ 变换为当前时间步的隐藏状态 $h_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换：

$$
  h_t = f(x_t, h_{t-1})
$$

- 接下来，编码器通过自定义函数 $q$ 将**各个时间步的隐藏状态变换为背景变量**：

$$
c=q(h1, ..., h_T)
$$

- 例如，当选择 $q(h_1,...,h_T ) = h_T $时，背景变量是输入序列最终时间步的隐藏状态 $h_T$。

以上描述的编码器是⼀个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时**取决于该时间步之前和之后的子序列**（包括当前时间步的输入），并编码了整个序列的信息。

#### :two:解码器

编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1,...,x_T$ 的信息。

给定训练样本中的输出序列 $y_1, y_2,..., y_{T^′} $，对每个时间步 $t^′$（符号与输入序列或编码器的时间步 $t$ 有区别），解码器输出 $y_{t^′}$ 的条件概率将基于之前的输出序列 $y_1,...,y_{t^′}$ 和背景变量 $c$，即：
$$
P(y_{t^′}|y_1,...y_{t^′-1},c)
$$

- 为此，我们可以使用另⼀个**循环神经网络**作为解码器。

- 在输出序列的时间步 $t^′$，解码器将上⼀时间步的输出以及背景变量 $c$ 作为输入，并将它们与上⼀时间步的隐藏状态变换为当前时间步的隐藏状态 $s_{t^′}$。因此，我们可以用函数 $g$ 表达**解码器隐藏层的变换**：

$$
s_{t^′}=g(y_{t^′-1},c,s_{t^′-1})
$$

- 有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算 $P(y_{t^′}|y_1,...y_{t^′-1},c)$ 。例如，基于当前时间步的解码器隐藏状态 $s_{t^′}$、上⼀时间步的输出以及背景变量 $c$ 来计算当前时间步输出 $y_{t^′}$ 的概率分布。

---



### §6.3 Seq2Seq模型训练与预测