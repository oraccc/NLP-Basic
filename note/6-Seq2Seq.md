## Seq2Seq



### §6.1 Seq2Seq简介

在自然语⾔处理的很多应用中，输入和输出都可以是不定长序列。以机器翻译为例，输入可以是⼀段不定长的英语文本序列，输出可以是一段不定长的法语文本序列，

> 例如：
>
> 英语输入：“They”、“are”、“watching”、“.”
>
> 法语输出：“Ils”、“regardent”、“.”

当输入和输出都是不定长序列时，我们可以使用编码器—解码器（encoder-decoder）或者序列到序列模型（**seq2seq模型**）。这两个模型本质上都用到了两个循环神经网络，分别叫做**编码器**和**解码器**。编码器用来分析输入序列，解码器用来生成输出序列。两个循环神经网络是**共同训练**的。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/Seq2Seq/translate.png" width="700" />

上图描述了使用编码器—解码器将上述英语句子翻译成法语句子的⼀种方法。

- 在训练数据集中，我们可以在每个句子后附上特殊符号“\<eos>”（end of sequence）以表示序列的终止。
- 编码器每个时间步的输入依次为英语句子中的单词、标点和特殊符号“\<eos>”。上图中使用了编码器**在最终时间步的隐藏状态**作为输⼊句子的表征或编码信息。
- 解码器**在各个时间步中**使用输入句子的编码信息和上个时间步的输出以及隐藏状态作为输入。
- 我们希望解码器在各个时间步能正确依次输出翻译后的法语单词、标点和特殊符号“\<eos>”。
- 需要注意的是，解码器在最初时间步的输入用到了⼀个表示序列开始的特殊符号“\<bos>”（beginning of sequence）。

---



### §6.2 编码器与解码器

#### :one:编码器

编码器的作用是把⼀个不定长的输入序列**变换成⼀个定长的背景变量** $c$，并在该背景变量中编码输入序列信息。

常用的编码器是**循环神经网络**。

- 让我们考虑批量大小为 $1$ 的时序数据样本。假设输入序列是 $x_1,...,x_T$，例如 $x_i$ 是输入句子中的第 $i$ 个词。

- 在时间步 $t$，循环神经网络将输入 $x_t$ 的特征向量 $x_t$ 和上个时间步的隐藏状态 $h_{t-1}$ 变换为当前时间步的隐藏状态 $h_t$。我们可以用函数 $f$ 表达循环神经网络隐藏层的变换：

$$
  h_t = f(x_t, h_{t-1})
$$

- 接下来，编码器通过自定义函数 $q$ 将**各个时间步的隐藏状态变换为背景变量**：

$$
c=q(h1, ..., h_T)
$$

- 例如，当选择 $q(h_1,...,h_T ) = h_T $时，背景变量是输入序列最终时间步的隐藏状态 $h_T$。

以上描述的编码器是⼀个单向的循环神经网络，每个时间步的隐藏状态只取决于该时间步及之前的输入子序列。我们也可以使用双向循环神经网络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时**取决于该时间步之前和之后的子序列**（包括当前时间步的输入），并编码了整个序列的信息。

#### :two:解码器

编码器输出的背景变量 $c$ 编码了整个输入序列 $x_1,...,x_T$ 的信息。

给定训练样本中的输出序列 $y_1, y_2,..., y_{T^′} $，对每个时间步 $t^′$（符号与输入序列或编码器的时间步 $t$ 有区别），解码器输出 $y_{t^′}$ 的条件概率将基于之前的输出序列 $y_1,...,y_{t^′}$ 和背景变量 $c$，即：
$$
P(y_{t^′}|y_1,...y_{t^′-1},c)
$$

- 为此，我们可以使用另⼀个**循环神经网络**作为解码器。

- 在输出序列的时间步 $t^′$，解码器将上⼀时间步的输出以及背景变量 $c$ 作为输入，并将它们与上⼀时间步的隐藏状态变换为当前时间步的隐藏状态 $s_{t^′}$。因此，我们可以用函数 $g$ 表达**解码器隐藏层的变换**：

$$
s_{t^′}=g(y_{t^′-1},c,s_{t^′-1})
$$

- 有了解码器的隐藏状态后，我们可以使用自定义的输出层和softmax运算来计算 $P(y_{t^′}|y_1,...y_{t^′-1},c)$ 。例如，基于当前时间步的解码器隐藏状态 $s_{t^′}$、上⼀时间步的输出$s_{t^′-1}$以及背景变量 $c$ 来计算当前时间步输出 $y_{t^′}$ 的概率分布。

---



### §6.3 Seq2Seq模型训练与预测

#### :one:训练模型

根据最大似然估计，我们可以最大化输出序列基于输入序列的条件概率


$$
P(y_1,...,y_{t^′-1}|x_1,...,x_T) = \prod_{t^′=1}^{T^′} P(y_{t^′}|y_1,...,y_{t^′-1},x_1,...,x_T) \\
=\prod_{t^′=1}^{T^′}P(y_{t^′}|y_1,...,y_{t^′-1},c)
$$

并得到该输出序列的损失

$$
-logP(y_1,...,y_{t^′-1}|x_1,...,x_T) =-\sum_{t^′=1}^{T^′}logP(y_{t^′}|y_1,...,y_{t^′-1},c)
$$

在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在上图所描述的模型预测中，我们需要将解码器在上⼀个时间步的输出作为当前时间步的输入。

与此不同，在训练中我们也可以将标签序列（训练集的真实输出序列）在上⼀个时间步的标签作为解码器在当前时间步的输入。这叫作**强制教学**（teacher forcing）。

#### :two:模型预测

以上介绍了如何训练输入和输出均为不定长序列的编码器—解码器。本节我们介绍如何使用编码器—解码器来预测不定长的序列。

在准备训练数据集时，我们通常会在样本的输入序列和输出序列后面分别附上⼀个特殊符号“\<eos>”表示序列的终止。为了便于讨论，假设解码器的输出是⼀段文本序列。设输出文本词典 $Y$（包含特殊符号“\<eos>”）的大小为 $|Y|$，输出序列的最大长度为$T^′$。所有可能的**输出序列⼀共有 $O(|y|^{T^′})$ 种**。这些输出序列中所有特殊符号“\<eos>”后面的子序列将被舍弃。



##### Method 1: 贪婪搜索

- 对于输出序列**任⼀时间步** $t^′$，我们从 $|Y|$ 个词中搜索出条件概率最大的词作为输出。

$$
y_{t^′}=argmax_{y \in Y}P(y|y_1,...,y_{t^′-1},c)
$$
- ⼀旦搜索出“\<eos>”符号，或者输出序列长度已经达到了最大长度$T^′$，便完成输出。

- 我们在描述解码器时提到，基于输入序列生成输出序列的条件概率是 $\prod_{t^′=1}^{T^′} P(y_{t^′}|y_1,...,y_{t^′-1},c)$ 我们将该条件概率最大的输出序列称为**最优输出序列**。

而**贪婪搜索的主要问题是不能保证得到最优输出序列**。

> 假设输出词典里面有“A”, “B”, “C”和“\<eos>”这4个词。下图中每个时间步下的4个数字分别代表了该时间步生成“A”, “B”, “C”和“\<eos>”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最大的词。因此，下图中将生成输出序列 `A, B, C, <eos>`。该输出序列的条件概率是 $0.5 × 0.4 × 0.4 × 0.6 = 0.048$。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/Seq2Seq/example1.png" width="350" />

> 接下来，观察下面演示的例子。与上图中不同，在时间步2中选取了条件概率第⼆大的词“C” 。由于时间步3所基于的时间步1和2的输出子序列由上图中的“A”“B”变为了下图中的“A”“C”，下图中时间步3生成各个词的条件概率发生了变化。我们选取条件概率最大的词“B”。此时时间步4所基于的前3个时间步的输出子序列为`A, C, B`，与上图中的`A, B, C`不同。因此，下图中时间步4生成各个词的条件概率也与上图中的不同。我们发现，此时的输出序列`A, C, B, <eos>`的条件概率是 $0.5 × 0.3 × 0.6 × 0.6 = 0.054$，大于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列`A, B, C, <eos>`并非最优输出序列。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/Seq2Seq/example2.png" width="350" />



##### Method 2: 穷举搜索

如果目标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：**穷举所有可能的输出序列**，输出条件概率最大的序列。

虽然穷举搜索可以得到最优输出序列，但它的计算开销 $O(|y|^{T^′})$ 很容易过大。

> 例如，当 $|Y| =10000$ 且 $T^′ = 10$ 时，我们将评估 $10000^{10}=10^{40}$ 个序列：这几乎不可能完成。

而贪婪搜索的计算开销是 $O(|y|*T^′)$  ，通常显著小于穷举搜索的计算开销。

> 例如，当 $|Y| =10000$ 且 $T^′ = 10$ 时，我们只需评估 $10000 * 10 = 10^5$个序列。



##### Method 3: 束搜索

束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个**束宽**（beam size）超参数。我们将它设为 $k$。

- 在时间步 1 时，选取当前时间步条件概率最大的 $k$ 个词，分别组成 $k$ 个候选输出序列的首词。
- 在之后的每个时间步，基于上个时间步的 $k$ 个候选输出序列，从 $k*|Y|$ 个可能的输出序列中选取条件概率最大的 $k$ 个，作为该时间步的候选输出序列。
- 最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“\<eos>”的序列，并将它们中所有特殊符号“\<eos>”后面的子序列舍弃，得到**最终候选输出序列的集合**。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/Seq2Seq/beam search.png" width="650" />

束宽为2，输出序列最大长度为3。

候选输出序列有A、C、AB、CE、ABD和CED。我们将根据这6个序列得出最终候选输出序列的集合。

在最终候选输出序列的集合中，我们取以下分数最高的序列作为输出序列
$$
\frac{1}{L^\alpha}logP(y_1,...,y_L) = \frac{1}{L^\alpha}\sum_{t^′=1}^{T^′}logP(y_{t^′}|y_1,...,y_{t^′-1},c)
$$
其中 $L$ 为最终候选序列长度，$\alpha$ ⼀般可选为0.75。

分母上的 $L^{\alpha}$ 是为了惩罚较长序列在以上分数中较多的对数相加项。

分析可知，束搜索的计算开销为$O(k*|y|^{T^′})$。这介于贪婪搜索和穷举搜索的计算开销之间。此外，贪婪搜索可看作是**束宽为 1 的束搜索**。束搜索通过灵活的**束宽 k 来权衡计算开销和搜索质量**。

---



### §6.4 BLEU得分

评价机器翻译结果通常使用**BLEU（Bilingual Evaluation Understudy）**(双语评估替补)。

对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。

具体来说，设词数为 $n$ 的子序列的精度为 $p_n$。它是预测序列与标签序列匹配词数为 $n$ 的子序列的数量与预测序列中词数为 $n$ 的子序列的数量之比。举个例子，假设标签序列为`A、B、C、D、E、F`，预测序列为`A、B、B、C、D`，那么：
$$
p_n=\frac{预测序列中的n元词组在标签序列中是否存在的个数}{预测序列n元词组的个数之和}
$$
所以$p_1=\frac{4}{5}$，以此类推，$p_2=\frac{3}{4},p_3=\frac{1}{3},p_4=0$。

设$len_{label},len_{pred}$分别为标签序列和预测序列的词数，那么BLEU的定义
$$
BLEU=exp(min(0,1-\frac{len_{label}}{len_{pred}}))\prod_{n=1}^{k}p_n^{\frac{1}{2^n}}
$$
其中 $k$ 是我们希望匹配的子序列的最大词数。可以看到**当预测序列和标签序列完全⼀致时，BLEU为1**。

因为匹配较长子序列比匹配较短子序列更难，BLEU对匹配**较长子序列的精度赋予了更大权重**。

>例如，当 $p_n$ 固定在 $0.5$ 时，随着 $n$ 的增⼤，
>
>$0.5^{\frac{1}{2}} \approx 0.7, 0.5^{\frac{1}{4}} \approx 0.84, 0.5^{\frac{1}{8}} \approx 0.92, 0.5^{\frac{1}{16}} \approx 0.96$

另外，模型预测较短序列往往会得到较高 $p_n$ 值。因此，上式中连乘项前面的系数是为了**惩罚较短的输出**而设的。

> 例如，当 $k = 2$ 时，假设标签序列为`A、B、C、D、E、F`，而预测序列为`A、 B`。虽然 $p_1 = p_2 = 1$，但惩罚系数 $exp(1-6/2) \approx 0.14$，因此BLEU也接近 $0.14$ 

---
