## 全局向量词嵌入 (GloVe)



### §3.1 GloVe简介

**GloVe的全称叫Global Vectors for Word Representation**，它是一个基于**全局词频**统计（count-based & overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了**单词之间一些语义特性**，比如相似性（similarity）、类比性（analogy）等我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

GloVe是一个**全局对数双线性回归模型**（global log bilinear regression model）。顾名思义，该模型用到了语料库的全局特征，即单词的共现频次矩阵，并且，其优化目标函数是对数线性的，并用回归的形式进行求解

---



### §3.2 GloVe的实现过程

#### :one:搭建共现矩阵

参考之前的词嵌入笔记，局域窗中的word-word共现矩阵可以挖掘语法和语义信息

>  例子：I like deep learning. I like NLP. I enjoy flying
>
>  以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}
>
>  可以得到一个共现矩阵(对称矩阵)

| counts       | I    | like | enjoy | deep | learning | NLP  | flying | .    |
| ------------ | ---- | ---- | ----- | ---- | -------- | ---- | ------ | ---- |
| **I**        | 0    | 2    | 1     | 0    | 0        | 0    | 0      | 0    |
| **like**     | 2    | 0    | 0     | 1    | 0        | 1    | 0      | 0    |
| **enjoy**    | 1    | 0    | 0     | 0    | 0        | 0    | 1      | 0    |
| **deep**     | 0    | 1    | 0     | 0    | 1        | 0    | 0      | 0    |
| **learning** | 0    | 0    | 0     | 1    | 0        | 0    | 0      | 1    |
| **NLP**      | 0    | 1    | 0     | 0    | 0        | 0    | 0      | 1    |
| **flying**   | 0    | 0    | 1     | 0    | 0        | 0    | 0      | 1    |
| **.**        | 0    | 0    | 0     | 0    | 1        | 1    | 1      | 0    |

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性

##### GloVe的共现矩阵

- [x] 根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）X，**矩阵中的每一个元素 $X_{ij}$代表单词 i 和上下文单词 j 在特定大小的上下文窗口（context window）内共同出现的次数。**
- [x] 一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离 d，提出了一个**衰减函数**（decreasing weighting）：decay=1/d 用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。



#### :two:构建词向量和共现矩阵的近似关系

构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：

$$
w_{i}^{T}\tilde{w_{j}} + b_{i} + \tilde{b_{j}} = log(X_{ij})
$$
其中，**$ w_{i}^{T} $和$ \tilde{w_{j}}$是我们最终要求解的词向量；**$b_{i}$ 和 $\tilde{b_{j}}$ 分别是两个词向量的bias term。



##### :bookmark_tabs:Extra: 此公式的推导

我们先定义一些变量：

- $X_{ij}$表示单词$j$出现在单词$i$的上下文中的次数；
- $X_i$表示单词$i$的上下文中所有单词出现的总次数，即$X_i=\sum^{k} X_{ik}$；
- $P_{ij}=P(j|i)=X_{ij}/X_i$，即表示单词$j$出现在单词$i$的上下文中的概率；

观察下方表格

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/GloVe/table.png" width = "550" >

> * 理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），我们可以使用它观察出两个单词$i$和j相对于单词$k$哪个更相关。
>
>   * ice和solid更相关，而stream和solid明显不相关，于是我们会发现$P(solid|ice)/P(solid|steam)$比1大更多。
>
>   * gas和steam更相关，而和ice不相关，那么$P(gas|ice)/P(gas|steam)$就远小于1；
>
>   * 当都有关 (water) 或者都没有关 (fashion) 的时候，两者的比例接近于1
>
> * 这个是很直观的。因此，**以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法**

于是为了捕捉上面提到的概率比例，我们可以构造如下函数：

我们可以构造如下函数：
$$
F(w_i,w_j,\tilde{w_k})=\frac{P_{ik}}{P_{jk}}
$$
其中，函数$F$的参数和具体形式未定，它有三个参数 $wi$, $wj$和 $\tilde{w_{k}}$，$w$和 $\tilde{w}$是不同的向量

> 一些Proof Sketch （完整版[见此](https://www.fanyeong.com/2018/02/19/glove-in-detail/)）
>
> * 因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是**作差**
> * 把左侧函数转换成两个向量的**内积**形式
>
> * $X$应该是个对称矩阵，单词和上下文单词其实是相对的
>   * 如果我们做如下交换：$w ↔ \tilde{w_k}$，$X ↔ X^T$公式应该保持不变，为了满足这个条件，我们要求函数F要满足**同态特性**
>
> * 公式应该满足**对称性**（交换$i$和$j$公式形式不变），最后的得到的公式如下：


$$
w_{i}^{T}\tilde{w_{j}} + b_{i} + \tilde{b_{j}} = log(X_{ij})
$$


#### :three:构建损失函数

有了词向量和共现矩阵之间的近似关系之后，便可以构建损失函数了
$$
J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T\tilde{w_j} + b_i + \tilde{b_j}-log(X_{ij}))^2
$$
这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数$f(X_{ij})$

此函数的作用:

> 我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的，那么我们希望：
> - 这些单词的权重要大于那些很少在一起出现的单词，所以这个函数要是**非递减函数**
> - 但我们也不希望这个权重过大（overweighted），当**到达一定程度之后应该不再增加**；
> - 如果两个单词没有在一起出现，也就是$X_{ij}=0$，那么他们应该不参与到 loss function 的计算当中去，也就是$f(x)$要满足$$f(0)=0$$

作者采用了如下形式的分段函数：
$$
f(x)=\begin{cases}
(x/x_{max})^{\alpha} & x<x_{max}\\
1 & otherwise
\end{cases}
$$
这个函数图像如下所示：这篇论文中的所有实验，$α$的取值都是0.75，而$x_{max}$取值都是100
