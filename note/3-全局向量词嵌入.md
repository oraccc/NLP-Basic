## 全局向量词嵌入 (GloVe)



### §3.1 GloVe简介

**GloVe的全称叫Global Vectors for Word Representation**，它是一个基于**全局词频**统计（count-based & overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了**单词之间一些语义特性**，比如相似性（similarity）、类比性（analogy）等我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

GloVe是一个**全局对数双线性回归模型**（global log bilinear regression model）。顾名思义，该模型用到了语料库的全局特征，即单词的共现频次矩阵，并且，其优化目标函数是对数线性的，并用回归的形式进行求解

---



### §3.2 GloVe的实现过程

#### :one:搭建共现矩阵

参考之前的词嵌入笔记，局域窗中的word-word共现矩阵可以挖掘语法和语义信息

>  例子：I like deep learning. I like NLP. I enjoy flying
>
>  以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}
>
>  可以得到一个共现矩阵(对称矩阵)

| counts       | I    | like | enjoy | deep | learning | NLP  | flying | .    |
| ------------ | ---- | ---- | ----- | ---- | -------- | ---- | ------ | ---- |
| **I**        | 0    | 2    | 1     | 0    | 0        | 0    | 0      | 0    |
| **like**     | 2    | 0    | 0     | 1    | 0        | 1    | 0      | 0    |
| **enjoy**    | 1    | 0    | 0     | 0    | 0        | 0    | 1      | 0    |
| **deep**     | 0    | 1    | 0     | 0    | 1        | 0    | 0      | 0    |
| **learning** | 0    | 0    | 0     | 1    | 0        | 0    | 0      | 1    |
| **NLP**      | 0    | 1    | 0     | 0    | 0        | 0    | 0      | 1    |
| **flying**   | 0    | 0    | 1     | 0    | 0        | 0    | 0      | 1    |
| **.**        | 0    | 0    | 0     | 0    | 1        | 1    | 1      | 0    |

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性

##### GloVe的共现矩阵

- [x] 根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）X，**矩阵中的每一个元素 $X_{ij}$代表单词 i 和上下文单词 j 在特定大小的上下文窗口（context window）内共同出现的次数。**
- [x] 一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离 d，提出了一个**衰减函数**（decreasing weighting）：decay=1/d 用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。



#### :two:构建词向量和共现矩阵的近似关系

构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：

$$
w_{i}^{T}\tilde{w_{j}} + b_{i} + \tilde{b_{j}} = log(X_{ij})
$$
其中，**$ w_{i}^{T} $和$ \tilde{w_{j}}$是我们最终要求解的词向量；**$b_{i}$ 和 $\tilde{b_{j}}$ 分别是两个词向量的bias term。



##### :bookmark_tabs:Extra: 公式的推导

我们先定义一些变量：

- $X_{ij}$表示单词$j$出现在单词$i$的上下文中的次数；
- $X_i$表示单词$i$的上下文中所有单词出现的总次数，即$X_i=\sum^{k} X_{ik}$；
- $P_{ij}=P(j|i)=X_{ij}/X_i$，即表示单词$j$出现在单词$i$的上下文中的概率；
