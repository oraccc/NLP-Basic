## 子词嵌入(fastText)



### §2.1 fastText简介

fastText是Facebook Research在2016年开源的一个**词向量及文本分类工具**。

在模型架构上跟word2vec非常相似（两者为同一个作者）。其实从另一种角度理解，fastText算法是word2vec的一种**衍生模型**。

#### 使用子词嵌入的动机

- [x] 英语单词通常有其内部结构和形成方式。例如，我们可以从“dog”“dogs”和“dogcatcher”的字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使用不同的后缀来改变词的含义。而且，这个关联可以推广至其他词汇。
- [x] 在word2vec中，我们并没有直接利用**构词学**中的信息。⽆论是在Skip-gram模型还是CBOW模型中，我们都将形态不同的单词用不同的向量来表示。
	> 例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。
- [x] 鉴于此，**fastText提出了子词嵌入(subword embedding)的方法**，从而试图将构词信息引入word2vec中的**CBOW**。

> 需要特别注意，一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。

---



### §2.2 字符级别的n-gram

word2vec把语料库中的每个单词当成原子，它会为每个单词生成一个向量，这忽略了单词内部的形态特征，如“apple”与“apples”，两个单词都有较多的公共字符，即它们的**内部形态类似**，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了

为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词

> 例如：对于“apple”，假设n的取值为3，则它的trigram有：
>
> "<ap","app","ppl","ple","le>"

其中<表示前缀，>表示后缀，我们可以使用这5个trigram的向量叠加来表示“apple”的词向量

**:question:使用n-gram表示词向量的优点？**

> - 对于**低频词**生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。
> - 对于训练词库之外的单词，仍然可以**构建它们的词向量**。我们可以叠加它们的字符级n-gram向量

---



### §2.3 fastText的模型架构与分析

#### 模型架构

模型架构如下图：

