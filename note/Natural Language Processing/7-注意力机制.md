## 注意力机制



### §7.1 注意力机制(Attention Mechanism)简介

在”[编码器—解码器（seq2seq）](https://github.com/oraccc/NLP-Basic/blob/master/note/Natural%20Language%20Processing/6-Seq2Seq.md)“⼀节里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。

现在，让我们再次思考上一节提到的翻译例子：

> 输入为英语序列`“They”,“are”,“watching”,“.”`，输出为法语序列`“Ils”,“regardent”,“.”`。
>
> 不难想到，解码器在生成输出序列中的每⼀个词时可能只需利用输入序列某⼀部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖`“They”,“are”`的信息来生成`“Ils”`，在时间步2则主要使用来自`“watching”`的编码信息⽣成`“regardent”`，最后在时间步3则直接映射句号`“.”`。

**这看上去就像是在解码器的每⼀时间步对输⼊序列中不同时间步的表征或编码信息分配不同的注意⼒⼀样。这也是注意⼒机制的由来。**

**仍然以循环神经⽹络为例，注意⼒机制通过对编码器所有时间步的隐藏状态做加权平均来得到背景变量。解码器在每⼀时间步调整这些权重，即注意⼒权重，从而能够在不同时间步分别关注输⼊序列中的不同部分并编码进相应时间步的背景变量。**