## 注意力机制



### §7.1 注意力机制(Attention Mechanism)简介

在 [编码器—解码器（seq2seq）](https://github.com/oraccc/NLP-Basic/blob/master/note/Natural%20Language%20Processing/6-Seq2Seq.md) ⼀节里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。

现在，让我们再次思考上一节提到的翻译例子：

> 输入为英语序列`“They”,“are”,“watching”,“.”`，输出为法语序列`“Ils”,“regardent”,“.”`。
>
> 不难想到，解码器在生成输出序列中的每⼀个词时可能只需利用输入序列某⼀部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖`“They”,“are”`的信息来生成`“Ils”`，在时间步2则主要使用来自`“watching”`的编码信息⽣成`“regardent”`，最后在时间步3则直接映射句号`“.”`。

这看上去就像是在解码器的每一时间步对输入序列中**不同时间步的表征或编码信息分配不同的注意力**⼀样。这也是注意力机制的由来。

仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态**做加权平均来得到背景变量**。解码器在**每一时间步调整这些权重**，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。

在注意力机制中，解码器的每⼀时间步将使用**可变的背景变量**。记 $c_{t'}$ 是解码器在时间步 $t'$ 的背景变量，那么解码器在该时间步的隐藏状态可以改写为：
$$
s_{t'}=g(y_{t'-1}, c_{t'}, s_{t'-1})
$$
问题的关键是

* 如何计算背景变量 $c_{t'}$
* 如何用背景变量来更新隐藏状态 $s_{t'}$

---



### §7.2 解编码器中的注意力机制

#### 计算背景变量

下图描绘了注意力机制如何为**解码器**在**时间步 2** 计算背景变量

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/attention/cal-background.png" width="550" />

* 函数 $a$ 根据**解码器在时间步 1 **的隐藏状态和**编码器在各个时间步**的隐藏状态计算softmax运算的输入
* softmax运算输出**概率分布**并对编码器各个时间步的隐藏状态做**加权平均**，从而得到背景变量

令编码器在时间步 $t$ 的隐藏状态为 $h_t$，且总时间步数为 $T$。那么解码器在时间步 $t′$ 的背景变量为所有编码器隐藏状态的加权平均：
$$
c_{t'} = \sum_{t=1}^T \alpha_{t't}h_{t}
$$

##### 矢量化计算背景变量

我们还可以对注意力机制采用更高效的矢量化计算。我们先定义，在上面的例子中，**查询项为解码器的隐藏状态**，**键项和值项均为编码器的隐藏状态**。

> 广义上，注意力机制的输入包括查询项以及⼀⼀对应的键项和值项，其中值项是需要加权平均的⼀组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。

让我们考虑⼀个常见的简单情形，即编码器和解码器的隐藏单元个数均为 $h$ ,并且函数 $a(s,h)=s^Th$

* 假设我们希望根据解码器单个隐藏状态 $s_{t'−1}$ 和编码器所有隐藏状态 $h_t, t = 1, . . . , T$ 来计算背景向量 $c_{t'}$ 
* 将查询矩阵 $Q$ 设为 $s_{t'-1}^{T}$, 并令键项矩阵 $K$ 和值项矩阵 $V$ 相同且第 $t$ 行均为 $h_t^T$
* 只需要矢量化计算 $softmax(QK^T)V$ 即可算出转置后的背景向量 $c_{t'}^T$

当查询项矩阵 $Q$ 的行数为 $n$ 时，上式将得到 $n$ 行的输出矩阵。输出矩阵与查询项矩阵在相同行上⼀⼀对应。



#### 更新隐藏状态

以门控循环单元（GRU）为例

对GRU的设计稍作修改，从而变换上⼀时间步 $t'−1$ 的输出 $y_{t'−1}$、隐藏状态 $s_{t'−1}$ 和当前时间步 $t'$ 的含注意力机制的背景变量 $c_{t'}$。

解码器在时间步 $t'$ 的隐藏状态为：
$$
s_{t'} = z_{t'} \odot s_{t'-1} + (1-z_{t'}) \odot \tilde{s}_{t'}
$$

其中的重置门、更新门和候选隐藏状态分别为
$$
r_{t'} = \sigma(W_{yr}y_{t'-1} + W_{sr}s_{t'-1} + W_{cr}c_{t'} + b_r) 
$$
$$
z_{t'} = \sigma(W_{yz}y_{t'-1} + W_{sz}s_{t'-1} + W_{cz}c_{t'} + b_z)
$$
$$
\tilde s_{t'} = tanh(W_{ys}y_{t'-1} + W_{ss}(s_{t'-1} \odot r_{t'}) + W_{cs}c_{t'} + b_s)
$$

---



### §7.3 进一步细解Attention本质

#### 从机器翻译说明Attention

首先以机器翻译作为例子讲解最常见的**Soft Attention模型**的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想。

> 如果拿机器翻译来解释这个Encoder-Decoder框架更好理解，比如输入的是英文句子：`Tom chase Jerry`，Encoder-Decoder框架逐步生成中文单词：`“汤姆”，“追逐”，“杰瑞”`。

在翻译`“杰瑞”`这个中文单词的时候，模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，**显然“Jerry”对于翻译成“杰瑞”更重要，但是模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。**

没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时**所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失**，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。

上面的例子中，如果引入Attention模型的话，应该在翻译`“杰瑞”`的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：

`（Tom,0.3）(Chase,0.2) (Jerry,0.5)`

**每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小**，这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。

同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词 $y_i$ 的时候，原先都是相同的中间语义表示 $C$ 会被替换成根据当前生成单词而不断变化的 $C_i$。理解Attention模型的关键就是这里，即由固定的中间语义表示 $C$ 换成了根据当前输出单词来调整成加入注意力模型的变化的 $C_i$。

增加了注意力模型的Encoder-Decoder框架理解起来如下图所示。

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/attention/ed-with-attention.png" width="550" />

每个 $C_i$ 可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下
$$
C_{汤姆} = g(0.6 * f_2(Tom), 0.2 * f_2(chase), 0.2 * f_2(Jerry))
$$
$$
C_{追逐} = g(0.2 * f_2(Tom), 0.7 * f_2(chase), 0.1 * f_2(Jerry))
$$
$$
C_{杰瑞} = g(0.3 * f_2(Tom), 0.2 * f_2(chase), 0.5 * f_2(Jerry))
$$
其中，$f_2$ 函数代表Encoder对输入英文单词的某种变换函数，比如Encoder是用的RNN模型的话，这个 $f_2$ 函数的结果往往是某个时刻输入 $x_i$ 后隐层节点的状态值；

$g$ 代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，$g$ 函数就是对构成元素加权求和，即下列公式：
$$
C_i = \sum _{j=1}^{L_x} a_{ij}h_{j}
$$
其中，$L_x$ 代表输入句子Source的长度，$a_ij$代表在Target输出第 $i$ 个单词时Source输入句子中第 $j$ 个单词的注意力分配系数，而 $h_j$ 则是Source输入句子中第 $j$ 个单词的语义编码。

> 假设下标i就是上面例子所说的`“汤姆”` ，那么 $L_x$ 就是3，$h_1=f(Tom), h_2=f(chase), h_3=f(Jerry)$ 分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是`0.6, 0.2, 0.2`，所以 $g$ 函数本质上就是个**加权求和函数**。 
