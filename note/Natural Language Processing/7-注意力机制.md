## 注意力机制



### §7.1 注意力机制(Attention Mechanism)简介

在 [编码器—解码器（seq2seq）](https://github.com/oraccc/NLP-Basic/blob/master/note/Natural%20Language%20Processing/6-Seq2Seq.md) ⼀节里，解码器在各个时间步依赖相同的背景变量来获取输入序列信息。当编码器为循环神经网络时，背景变量来自它最终时间步的隐藏状态。

现在，让我们再次思考上一节提到的翻译例子：

> 输入为英语序列`“They”,“are”,“watching”,“.”`，输出为法语序列`“Ils”,“regardent”,“.”`。
>
> 不难想到，解码器在生成输出序列中的每⼀个词时可能只需利用输入序列某⼀部分的信息。例如，在输出序列的时间步1，解码器可以主要依赖`“They”,“are”`的信息来生成`“Ils”`，在时间步2则主要使用来自`“watching”`的编码信息⽣成`“regardent”`，最后在时间步3则直接映射句号`“.”`。

这看上去就像是在解码器的每一时间步对输入序列中**不同时间步的表征或编码信息分配不同的注意力**⼀样。这也是注意力机制的由来。

仍然以循环神经网络为例，注意力机制通过对编码器所有时间步的隐藏状态**做加权平均来得到背景变量**。解码器在**每一时间步调整这些权重**，即注意力权重，从而能够在不同时间步分别关注输入序列中的不同部分并编码进相应时间步的背景变量。

在注意力机制中，解码器的每⼀时间步将使用**可变的背景变量**。记 $c_{t'}$ 是解码器在时间步 $t'$ 的背景变量，那么解码器在该时间步的隐藏状态可以改写为：
$$
s_{t'}=g(y_{t'-1}, c_{t'}, s_{t'-1})
$$
问题的关键是

* 如何计算背景变量 $c_{t'}$
* 如何用背景变量来更新隐藏状态 $s_{t'}$

---



### §7.2 解编码器中的注意力机制

