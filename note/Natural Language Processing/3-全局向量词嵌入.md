## 全局向量词嵌入 (GloVe)



- [全局向量词嵌入 (GloVe)](#全局向量词嵌入-glove)
  - [§3.1 GloVe简介](#31-glove简介)
  - [§3.2 GloVe的实现过程](#32-glove的实现过程)
    - [:one:搭建共现矩阵](#one搭建共现矩阵)
      - [GloVe的共现矩阵](#glove的共现矩阵)
    - [:two:构建词向量和共现矩阵的近似关系](#two构建词向量和共现矩阵的近似关系)
      - [:bookmark_tabs:Extra: 此公式的推导](#bookmark_tabsextra-此公式的推导)
    - [:three:构建损失函数](#three构建损失函数)
    - [:four:训练GloVe](#four训练glove)
  - [§3.3 GloVe与其他模型的比较](#33-glove与其他模型的比较)
    - [相较于LSA](#相较于lsa)
    - [相较于Word2Vec](#相较于word2vec)

### §3.1 GloVe简介

> :memo: [Jupyter Notebook 代码](https://github.com/oraccc/NLP-Basic/blob/master/code/3-glove.ipynb)

**GloVe的全称叫Global Vectors for Word Representation**，它是一个基于**全局词频**统计（count-based & overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了**单词之间一些语义特性**，比如相似性（similarity）、类比性（analogy）等我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

GloVe是一个**全局对数双线性回归模型**（global log bilinear regression model）。顾名思义，该模型用到了语料库的全局特征，即单词的共现频次矩阵，并且，其优化目标函数是对数线性的，并用回归的形式进行求解

---



### §3.2 GloVe的实现过程

#### :one:搭建共现矩阵

参考之前的词嵌入笔记，局域窗中的word-word共现矩阵可以挖掘语法和语义信息

>  例子：I like deep learning. I like NLP. I enjoy flying
>
>  以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}
>
>  可以得到一个共现矩阵(对称矩阵)

| counts       | I    | like | enjoy | deep | learning | NLP  | flying | .    |
| ------------ | ---- | ---- | ----- | ---- | -------- | ---- | ------ | ---- |
| **I**        | 0    | 2    | 1     | 0    | 0        | 0    | 0      | 0    |
| **like**     | 2    | 0    | 0     | 1    | 0        | 1    | 0      | 0    |
| **enjoy**    | 1    | 0    | 0     | 0    | 0        | 0    | 1      | 0    |
| **deep**     | 0    | 1    | 0     | 0    | 1        | 0    | 0      | 0    |
| **learning** | 0    | 0    | 0     | 1    | 0        | 0    | 0      | 1    |
| **NLP**      | 0    | 1    | 0     | 0    | 0        | 0    | 0      | 1    |
| **flying**   | 0    | 0    | 1     | 0    | 0        | 0    | 0      | 1    |
| **.**        | 0    | 0    | 0     | 0    | 1        | 1    | 1      | 0    |

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性

##### GloVe的共现矩阵

- [x] 根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）$X$，**矩阵中的每一个元素 $X_{ij}$代表单词 $i$ 和上下文单词 $j$ 在特定大小的上下文窗口（context window）内共同出现的次数。**
- [x] 一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离 d，提出了一个**衰减函数**（decreasing weighting）：$decay=1/d$ 用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。



#### :two:构建词向量和共现矩阵的近似关系

构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：

$$
w_{i}^{T}\tilde{w_{j}} + b_{i} + \tilde{b_{j}} = log(X_{ij})
$$

其中，**$w_{i}^{T}$ 和 $\tilde{w_{j}}$是我们最终要求解的词向量；**$b_{i}$ 和 $\tilde{b_{j}}$ 分别是两个词向量的bias term。



##### :bookmark_tabs:Extra: 此公式的推导

我们先定义一些变量：

- $X_{ij}$表示单词 $j$ 出现在单词 $i$ 的上下文中的次数；
- $X_i$表示单词 $i$ 的上下文中所有单词出现的总次数，即$X_i=\sum^{k} X_{ik}$；
- $P_{ij}=P(j|i)=X_{ij}/X_i$，即表示单词 $j$ 出现在单词 $i$ 的上下文中的概率；

观察下方表格

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/GloVe/table.png" width = "550" >

> * 理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），我们可以使用它观察出两个单词$i$和$j$相对于单词$k$哪个更相关。
>
>   * ice和solid更相关，而stream和solid明显不相关，于是我们会发现$P(solid|ice)/P(solid|steam)$比1大更多。
>
>   * gas和steam更相关，而和ice不相关，那么$P(gas|ice)/P(gas|steam)$就远小于1；
>
>   * 当都有关 (water) 或者都没有关 (fashion) 的时候，两者的比例接近于1
>
> * 这个是很直观的。因此，**以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法**

于是为了捕捉上面提到的概率比例，我们可以构造如下函数：

我们可以构造如下函数：

$$
F(w_i,w_j,\tilde{w_k})=\frac{P_{ik}}{P_{jk}}
$$

其中，函数$F$的参数和具体形式未定，它有三个参数 $wi$, $wj$和 $\tilde{w_{k}}$，$w$和 $\tilde{w}$是不同的向量

> 一些Proof Sketch （完整版[见此](https://www.fanyeong.com/2018/02/19/glove-in-detail/)）
>
> * 因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是**作差**
> * 把左侧函数转换成两个向量的**内积**形式
>
> * $X$应该是个对称矩阵，单词和上下文单词其实是相对的
>   * 如果我们做如下交换：$w \rightarrow \tilde{w_k}$，$X \rightarrow X^T$公式应该保持不变，为了满足这个条件，我们要求函数$F$要满足**同态特性**
>
> * 公式应该满足**对称性**（交换$i$和$j$公式形式不变），最后的得到的公式如下：


$$
w_{i}^{T}\tilde{w_{j}} + b_{i} + \tilde{b_{j}} = log(X_{ij})
$$


#### :three:构建损失函数

有了词向量和共现矩阵之间的近似关系之后，便可以构建损失函数了

$$
J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T\tilde{w_j} + b_i + \tilde{b_j}-log(X_{ij}))^2
$$

这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数$ f(X_{ij}) $

此函数的作用:

> 我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的，那么我们希望：
> - 这些单词的权重要大于那些很少在一起出现的单词，所以这个函数要是**非递减函数**
> - 但我们也不希望这个权重过大（overweighted），当**到达一定程度之后应该不再增加**；
> - 如果两个单词没有在一起出现，也就是$X_{ij}=0$，那么他们应该不参与到 loss function 的计算当中去，也就是$f(x)$要满足$f(0)=0$

作者采用了如下形式的分段函数：

$$
f(x)=\begin{cases}
(x/x_{max})^{\alpha} & x<x_{max} \\
1 & otherwise
\end{cases}
$$

这个函数图像如下所示：这篇论文中的所有实验，$\alpha$的取值都是0.75，而$ x_{max} $取值都是100

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/GloVe/weighting%20function.jpg" width="400" />



#### :four:训练GloVe

虽然很多人声称GloVe是一种无监督的学习方式，但其实它还是有label的，这个label就是$log⁡(X_{ij})$，而公式中的向量$w$和$\tilde{w}$就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。

梯度下降算法的使用：

* 采用了**AdaGrad的梯度下降算法**，对矩阵X中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。
* 最终学习得到的是两个vector是$w$和$\tilde{w}$，因为$X$是对称的，所以从原理上讲$w$和$\tilde{w}$是也是对称的，它们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和$w+\tilde{w}$作为最终的vector
  * 两者的初始化不同相当于**加了不同的随机噪声，所以能提高鲁棒性**



论文的训练结果图

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/GloVe/experiments.jpg" width="700" />

> 一共采用了三个指标：语义准确度，语法准确度以及总体准确度

---



### §3.3 GloVe与其他模型的比较

#### 相较于LSA

潜在语义分析（Latent Semantic Analysis）是一种无监督的机器学习方式，通过分析文本内容来获取文本“隐藏主题”，也就是文章的“表示”，一旦能获取文本的“表示”就可以进行文本之间的相似度的计算，进而实现文本聚类等应用，LSA的具体分析[见此](https://zhuanlan.zhihu.com/p/530634827)。LSA是一种比较早的count-based的词向量表征工具，也基于co-occurance matrix的

* 采用了基于奇异值分解（SVD）的矩阵分解技术对大矩阵进行降维，而SVD的复杂度是很高的，所以它的**计算代价**比较大。

* LSA对所有单词的统计权重都是一致的

#### 相较于Word2Vec

Word2Vec最大的缺点则是没有充分利用所有的语料

---
