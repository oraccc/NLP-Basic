## 子词嵌入(fastText)

- [子词嵌入(fastText)](#子词嵌入fasttext)
  - [§2.1 fastText简介](#21-fasttext简介)
    - [使用子词嵌入的动机](#使用子词嵌入的动机)
  - [§2.2 字符级别的n-gram](#22-字符级别的n-gram)
  - [§2.3 fastText的模型架构与分析](#23-fasttext的模型架构与分析)
    - [:one:模型架构](#one模型架构)
    - [:two:模型分析](#two模型分析)
  - [§2.4 fastText与 Word2Vec 的比较](#24-fasttext与-word2vec-的比较)
    - [:one:相同点](#one相同点)
    - [:two:不同点](#two不同点)

### §2.1 fastText简介

> :memo: [Jupyter Notebook 代码](https://github.com/oraccc/NLP-Basic/blob/master/code/Natural%20Language%20Processing/2-fasttext.ipynb)

fastText是Facebook Research在2016年开源的一个**词向量及文本分类工具**。

在模型架构上跟word2vec非常相似（两者为同一个作者）。其实从另一种角度理解，fastText算法是word2vec的一种**衍生模型**。

#### 使用子词嵌入的动机

- [x] 英语单词通常有其内部结构和形成方式。例如，我们可以从“dog”“dogs”和“dogcatcher”的字面上推测它们的关系。这些词都有同⼀个词根“dog”，但使用不同的后缀来改变词的含义。而且，这个关联可以推广至其他词汇。

- [x] 在word2vec中，我们并没有直接利用**构词学**中的信息。⽆论是在Skip-gram模型还是CBOW模型中，我们都将形态不同的单词用不同的向量来表示。
	
	> 例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。
	
- [x] 鉴于此，**fastText提出了子词嵌入(subword embedding)的方法**，从而试图将构词信息引入word2vec中的**CBOW**。

> 需要特别注意，一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。

---



### §2.2 字符级别的n-gram

word2vec把语料库中的每个单词当成原子，它会为每个单词生成一个向量，这忽略了单词内部的形态特征，如“apple”与“apples”，两个单词都有较多的公共字符，即它们的**内部形态类似**，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了

为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词

> 例如：对于“apple”，假设n的取值为3，则它的trigram有：
>
> "<ap","app","ppl","ple","le>"

其中<表示前缀，>表示后缀，我们可以使用这5个trigram的向量叠加来表示“apple”的词向量

**:question:使用n-gram表示词向量的优点？**

> - 对于**低频词**生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。
> - 对于训练词库之外的单词，仍然可以**构建它们的词向量**。我们可以叠加它们的字符级n-gram向量
> - 词袋模型不能考虑词之间的顺序，而 fastText 通过引入 subword n-gram information 的技巧,来捕获一定的局部序列**词序信息**

---



### §2.3 fastText的模型架构与分析

#### :one:模型架构

模型架构如下图，与Word2Vec的CBOW模型架构十分相似：

<img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/fastText/fastText.png" width="450">

* 此架构图没有展示词向量的训练过程

* 与CBOW的**相似之处**
  * 和CBOW一样，fastText模型也只有三层：输入层、隐含层（全局平均池化GAP）、输出层（Hierarchical Softmax）
  * 输入都是多个**用向量表示的单词**
  * 输出都是一个特定的target
  * 隐含层都是对多个词向量的叠加平均

* 与CBOW的**不同之处**
  * CBOW的输入是目标单词的上下文，fastText的输入是**多个单词及其n-gram特征**，这些特征用来表示单个文档
  * CBOW的输入单词被one-hot编码过，fastText的输入特征是被embedding过
  * CBOW的输出是目标词汇，fastText的输出是**文档对应的类标**

* fastText在输入时，将单词的字符级别的n-gram向量作为**额外的特征**；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间

  * **压缩模型**的建议

    * **采用 hash-trick**。由于`n-gram`原始的空间太大，可以用某种`hash`函数将其映射到固定大小的`buckets`中去，哈希到同一个桶的所有n-gram共享一个embedding vector,从而实现内存可控

    <img src="https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/fastText/hash-trick.png" width="550" />

#### :two:模型分析

(a) 模型的前半部分（即从输入层输入到隐含层输出部分）

* 生成用来表征文档的向量：**叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。**
* 叠加词向量背后的思想就是传统的**词袋法**，即将文档看成一个由词构成的集合

(b) 模型的后半部分（即从隐含层输出到输出层输出）

* 一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量

**:star2:(c) 核心思想**

* **将整篇文档的词及n-gram向量叠加平均得到文档向量**，然后使用文档向量做softmax多分类。
  * 两个核心技巧：**字符级n-gram特征**的引入以及**分层Softmax**分类


(d) 模型的分类效果

- fastText用单词的embedding叠加获得的文档向量，词向量的重要特点就是**向量的距离**可以用来衡量单词间的语义相似程度
- 使用**词embedding**而非词本身作为特征，这是fastText效果好的一个原因；另一个原因就是**字符级n-gram**特征的引入对分类效果会有一些提升 

---



### §2.4 fastText与 Word2Vec 的比较

#### :one:相同点

- 模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。
- 都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的速度。

#### :two:不同点

|                                | Word2Vec                                                     | fastText                                                     |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **输入**                       | one-hot形式的单词的向量                                      | embedding过的单词的词向量和**n-gram**向量                    |
| **输出**                       | 对应的是每一个term,计算某term概率最大                        | 对应的是分类的标签                                           |
| **hierarchical softmax的使用** | CBOW中h-softmax的叶子节点是词和词频, word2vec的目的是得到词向量，该词向量最终是在输入层得到的，输出层对应的h-softmax也会生成一系列的向量，但是最终都被抛弃，不会使用 | fastText中h-softmax叶子节点里是类标和类标的频数，fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label |



**:star2:层次Softmax细解**

- 层次softmax是⼀种使用**最优二叉树(哈夫曼树)**结构替代网络原有输出层(全连接层)的方式。将一个全局多分类的问题，转化成为了**若干个二元分类问题**，从而将计算复杂度从`O(V)`降到`O(logV)`。每个二元分类问题，由一个基本的逻辑回归单元来实现。
- 提升训练效率的内在原理: 
  - 在训练阶段，由于⼆叉树是根据预先统计的每个标签数量的占比构造的哈夫曼树（最优⼆叉树），根据哈夫曼树的性质，**使得占比最大的标签节点路径最短**，又因为路径中的节点代表参数量，也就意味着这种方式需要更新的参数最少，因此提升训练速度。
- 该方式对模型推断(预测)是否有影响:
  - 在预测阶段，相比全连接层速度略有提升
- 是否存在⼀定弊端
  - 因为最优⼆叉树的节点中存储参数，而样本数量最多的标签对应的参数又最少，可能出现在**某些类别上欠拟合**，影响模型准确率。因此，如果不是存在大量目标类别产生的训练低效，首选具有全连接层的输出层



**:question:fastText的优点？**

> - **适合大型数据+高效的训练速度**：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”
>   - fast的原因?
>     - 模型结构简单，参数量相比大型模型(如BERT)较少，即提高训练效率又提高推断效率。
>     - 当业务场景中存在大量目标类别时，fastText的输出层使用层次softmax提升训练效率。
> - **支持多语言表达**：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText的性能要比word2vec工具明显好上不少，也比其他目前最先进的词态词汇表征要好。
> - **专注于文本分类**，在许多标准问题上表现要好（例如文本倾向性分析或标签预测）。

---
