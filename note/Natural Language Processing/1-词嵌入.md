## 词嵌入

**把词映射为实数域向量的技术**也叫词嵌入（word embedding）

近年来，词嵌⼊已逐渐成为自然语⾔处理的基础知识

早期是**基于规则**的方法进行转化，而现代的方法是**基于统计机器学习**的方法

- [词嵌入](#词嵌入)
  - [§1.1 离散表示](#11-离散表示)
    - [:one:One-hot编码（独热编码）](#oneone-hot编码独热编码)
    - [:two:词袋模型(BOW)](#two词袋模型bow)
    - [:three:TF-IDF](#threetf-idf)
    - [:four:n-gram模型](#fourn-gram模型)
    - [:five: 离散表示存在的问题](#five-离散表示存在的问题)
  - [§1.2 分布式表示](#12-分布式表示)
    - [:one:共现矩阵](#one共现矩阵)
  - [§1.3 神经网络表示](#13-神经网络表示)
    - [:one: NNLM(Neural Network Language Model)](#one-nnlmneural-network-language-model)
    - [:two:Word2Vec](#twoword2vec)
      - [CBOW](#cbow)
      - [Skip-gram](#skip-gram)
      - [针对Skip-gram计算复杂度的优化方法](#针对skip-gram计算复杂度的优化方法)

### §1.1 离散表示

#### :one:One-hot编码（独热编码）

特征工程中常用的方式，步骤如下

- [x] 构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1

- [x] 每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示


> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> One-hot表示为：
>
> John: **[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> likes: **[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]** 
>
> ...

**:question:为什么使用独热编码？**

>  独热编码将离散特征的取值扩展到了**欧式空间**，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用独热编码，**会让特征之间的距离计算更加合理**。

**:question:使用独热编码表示文本的缺点？**

> - 随着语料库的增加，数据特征的维度会越来越大，产生一个**维度很高**，又很**稀疏**的矩阵。 
> -  这种表示方法的分词顺序和在句子中的**顺序**是无关的，不能保留词与词之间的**关系信息**。



#### :two:词袋模型(BOW)

像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式**不考虑文法以及词的顺序**

文档的向量表示可以直接将各词的词向量表示**加和**

> 例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 同样得到词典：{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, ...} 
>
> 第一句的向量表示为：**[1,2,1,1,1,0,0,0,1,1]**，其中的2表示**likes**在该句中出现了2次，依次类推

**:question:词袋模型表示文本的缺点？**

> - 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
> - 词与词之间是没有**顺序关系**的。



#### :three:TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是**词频**(Term Frequency)，IDF意思是**逆文本频率指数**(Inverse Document Frequency)

- [x] 字词的重要性随着它在**文件中**出现的次数成正比增加，但同时会随着它在**语料库**中出现的频率成反比下降。
- [x] 一个词语在**一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章**。

计算公式（分母加1是为了防止分母为0）

$$
TF_{w} = \frac{在某一类词条w出现的次数}{此类中所有的词条数目}
$$

$$
IDF = log(\frac{语料库的总文档数}{包含词条w的文档总数+1})
$$

$$
TF-IDF = TF * IDF
$$

**:question:使用TF-IDF的缺点?**

> * 仅以“词频”度量词的重要性，后续构成文档的特征值序列，词之间各自独立，**无法反映序列信息**
> * 易受数据集偏斜的影响，如某一类别的文档偏多，会导致IDF低估
>   * 处理方式：增加类别权重
> * 没有考虑类内、类间分布偏差（被用于特征选择时）
>   * 比如只有2类数据，文档总数200，类1,类2各100个文档
>   * term1只出现在类1的所有100个文档，在类1出现总次数500；term2在类1出现次数也是500，但是类1和类2各有50个文档出现term2
>   * 此时对类1，计算两个term得到的TF-IDF结果是一样的；无法反映term1对类1的重要性



#### :four:n-gram模型

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是**滑窗的大小**

- [x] 例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。
- [x] 该模型**考虑了词的顺序**。

>  例子：John likes to watch movies. Mary likes too. John also likes to watch football games.
>
> 可以构造一个词典，{"John likes”: 1, "likes to”: 2, "to watch”: 3, "watch movies”: 4, "Mary likes”: 5, "likes too”: 6, "John also”: 7, "also likes”: 8, “watch football”: 9, "football games": 10}
>
> 那么第一句的向量表示为：**[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]**，其中第一个1表示John likes在该句中出现了1次，依次类推。

**:question:使用n-gram的缺点？**

> * 随着n的大小增加，词表会成指数型膨胀，会越来越大
>
> * 无法建模更远的依赖关系
> * 无法建模出词之间的相似度
>   * 比如在训练集中出现过句子"The cat is walking in the bedroom"，那么在碰到句子"A dog was running in a room"时，理应认为其出现的概率也是类似的，因为词"dog"和"cat"的相似性。但是N-gram并不能捕捉到这样的相似性
> * 泛化能力不够强，对于训练集中没有出现过的n元组条件概率为0的情况，只能用平滑法或者回退法赋予它们概率



#### :five: 离散表示存在的问题

- 无法衡量词向量之间的关系。
- 词表的维度随着语料库的增长而膨胀。
- n-gram词序列随语料库增长呈指数型膨胀，更加快。
- 离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的

---



### §1.2 分布式表示

**用一个词附近的其它词来表示该词**，这是现代统计自然语言处理中最有创见的想法之一。当初科学家发明这种方法是基于人的语言表达，认为一个词是由这个词的周边词汇一起来构成精确的语义信息。

#### :one:共现矩阵

词文档的共现矩阵主要用于发现主题(topic)

局域窗中的word-word共现矩阵可以挖掘语法和语义信息

>  例子：I like deep learning. I like NLP. I enjoy flying
>
> 以上三句话，设置滑窗为2，可以得到一个词典：{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}
>
> 可以得到一个共现矩阵(对称矩阵)

| counts       | I    | like | enjoy | deep | learning | NLP  | flying | .    |
| ------------ | ---- | ---- | ----- | ---- | -------- | ---- | ------ | ---- |
| **I**        | 0    | 2    | 1     | 0    | 0        | 0    | 0      | 0    |
| **like**     | 2    | 0    | 0     | 1    | 0        | 1    | 0      | 0    |
| **enjoy**    | 1    | 0    | 0     | 0    | 0        | 0    | 1      | 0    |
| **deep**     | 0    | 1    | 0     | 0    | 1        | 0    | 0      | 0    |
| **learning** | 0    | 0    | 0     | 1    | 0        | 0    | 0      | 1    |
| **NLP**      | 0    | 1    | 0     | 0    | 0        | 0    | 0      | 1    |
| **flying**   | 0    | 0    | 1     | 0    | 0        | 0    | 0      | 1    |
| **.**        | 0    | 0    | 0     | 0    | 1        | 1    | 1      | 0    |

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性

**:question:共现矩阵存在的问题？**

> - 向量维数随着词典大小线性增长。
> - 存储整个词典的空间消耗非常大。
> - 一些模型如文本分类模型会面临稀疏性问题。
> - **模型会欠稳定，每新增一份语料进来，稳定性就会变化。**

---



### §1.3 神经网络表示

#### :one: NNLM(Neural Network Language Model)

NNLM (Neural Network Language model)神经网络语言模型是03年提出来的，通过训练得到中间产物--**词向量矩阵**，这就是我们要得到的文本表示向量矩阵。

* NNLM说的是定义一个前向窗口大小，其实和上面提到的窗口是一个意思。把这个窗口中最后一个词当做y，把之前的词当做输入x，通俗来说就是**预测这个窗口中最后一个词出现概率的模型**。

模型结构

<img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/NNLM.jpg" width = "450"/>

> * input层是一个前向词的输入，是经过**one-hot编码**的词向量表示形式，具有V*1的矩阵。
> * 矩阵M是投影矩阵，也就是稠密词向量表示，在神经网络中是**w参数矩阵**，采用线性映射将one-hot表示投影到稠密词向量表示。
> * output层(softmax)是前向窗中需要预测的词。
> * 通过BP＋SGD（随机梯度下降）得到最优的投影矩阵，这就是NNLM的中间产物，也是我们所求的文本表示矩阵，**通过NNLM将稀疏矩阵投影到稠密向量矩阵中。**



#### :two:Word2Vec

> :memo: [Jupyter Notebook 代码](https://github.com/oraccc/NLP-Basic/blob/master/code/Natural%20Language%20Processing/1-word-embedding.ipynb)

谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。

Word2Vec实际是一种浅层的神经网络模型，其基本思想是用**目标词 w 和其上下文 context(w) 之间相互预测**，在这个过程中训练得到词典中词的向量。因为是相互预测，所以就有两种不同的模型来实现这个算法：

* 一种是利用上下文 context(w) 来预测目标词 w 的**CBOW(continuous vag-of-words)连续词袋模型**
* 一种是利用目标词 w 来预测它的上下文的词 context(w)的**skip-gram模型**

CBOW与Skip-gram的模型互为镜像。

<img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/word2vec.png" width = "500"/>

Word2Vec和NNLM很类似，但比NNLM简单。

##### CBOW

CBOW获得中间词两边的的上下文，**然后用周围的词去预测中间的词**，把中间词当做y，把窗口中的其它词当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化，而求得的**权重矩阵就是文本表示词向量的结果**。

模型图：

<img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/cbow.png" width = "400"/>

> 这里输入层是由one-hot编码组成的输入上下文，窗口大小为C，词汇表大小为V。隐藏层是N维的向量。最后输出层是也被one-hot编码的输出单词y。被one-hot编码的输入向量通过一个**V × N**维的权重矩阵**W**连接到隐藏层；隐藏层通过一个**N × V**的权重矩阵**W′**连接到输出层。

损失函数的定义

>  损失函数就是给定输入上下文的输出单词的条件概率
$$
E = -logP(w_{O}|w_{I})
$$



##### Skip-gram

Skip-gram是通过当前词来预测窗口中上下文词出现的概率模型，把当前词当做x，把窗口中其它词当做y，依然是通过一个隐层接一个Softmax激活函数来预测其它词的概率

模型图：

<img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/skip-gram.png" width = "400"/>

> 输入层与隐藏层之间的**权重矩阵W的第i行代表词汇表中第i个单词的权重**,这个权重矩阵W就是我们需要学习的目标（同W′），因为这个权重矩阵包含了词汇表中所有单词的权重信息

损失函数的定义

> 这个损失函数就是输出单词组的条件概率
$$
E=-log(w_{O,1}, w_{O,2},...,w_{O,C}|w_{I})
$$

##### 针对Skip-gram计算复杂度的优化方法

* **层次Softmax (Hierarchical Softmax)**
  
  * 因为如果单单只是接一个softmax激活函数，计算量还是很大的，有多少词就会有多少维的权重矩阵，所以这里就提出层次Softmax，使用**Huffman Tree**来编码输出层的词典，相当于平铺到各个叶子节点上，瞬间把维度降低到了树的深度，可以看如下图所示。这棵树**把词频的词放到靠近根节点的叶子节点处**，每一次只要做二分类计算，计算路径上所有非叶子节点词向量的贡献即可。
  
    <img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/h-softmax.png" width = "400"/>
  
  * 从根结点开始，每个中间结点（标记成灰色）都是一个逻辑回归单元，根据它的输出来选择下一步是向左走还是向右走。下图示例中实际上走了一条“左-左-右”的路线，从而找到单词`w₂`。**而最终输出单词`w₂`的概率，等于中间若干逻辑回归单元输出概率的连乘积**。
    $$
    p(w=w_{O}) = \prod_{j=1}^{L(w)-1} \sigma (⟦n(w,j+1)=ch(n(w,j))⟧ \cdot v_{n(w,j)}^{'T}h)
    $$
  
    * `⟦x⟧`是一个特殊的函数，**如果下一步需要向左走其函数值定义为`1`，向右则取`-1`**。在训练时我们知道最终输出叶子结点，并且从根结点到叶子结点的每一步的路径也是确定的。
    * `v'` 是每个内部结点（逻辑回归单元）对应的一个向量，这个向量可以在训练过程中学习和更新。
    * `h` 是网络中隐藏层的输出。
  
* **负例采样 (Negative Sampling)**

  * Vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。

  * 一种优化方式是，在正确单词以外的**负样本中进行采样**，最终目的是**为了减少负样本的数量**，达到减少计算量效果。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本**仅仅更新一小部分**的权重，这样就会降低梯度下降过程中的计算量

     > 例子：当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative word”。
     >
     > 在这9999个结点中选择部分（比如5个）和“positive word”一起更新，而不是全部更新。

  * 负样本生成方式：将词典中的每一个词对应一条线段，所有词组成了[0，1］间的剖分，如下图所示，然后每次随机生成一个[1, M-1]间的整数，看落在哪个词对应的部分上就选择哪个词，最后会得到一个负样本集合
  
    <img src = "https://raw.githubusercontent.com/oraccc/NLP-Basic/master/img/word-embedding/n-sample.jpg" width = "450"/>

**:question:Word2Vec的缺点？**

> - 对每个local context window单独训练，没有利用包含在**global co-currence**矩阵中的统计信息。
> - 对**多义词**无法很好的表示和处理，因为使用了唯一的词向量
> - 是一种静态的方式，虽然通用性强，但是无法**针对特定任务做动态优化**

**:star2:Word2Vec相较于One-hot编码的优势？**

> * 虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词向量无法准确表达不同词之间的相似度，
>   * 例如余弦相似度，由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。
>
> * Word2Vec的提出正是为了解决上面这个问题。它将每个词表示成⼀个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系。

---
