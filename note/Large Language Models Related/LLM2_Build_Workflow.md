## LLM构建流程

OpenAI 所使用的大规模语言模型构建流程如下图1所示。主要包含四个阶段：**预训练、有监督微调、奖励建模、强化学习**。这四个阶段都需要不同规模数据集合以及不同类型的算法，会产出不同类型的模 型，同时所需要的资源也有非常大的差别。

<img src="..\..\img\llm-basic\openai-flow.png" alt="图片" style="zoom:50%;" />

### 预训练（Pretraining）

该阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、GitHub、 论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用由数千块高性能 GPU 和高速网络组成超级计算机，花费数十天完成深度神经网络参数训练，构建基础语言模型 （Base Model）。

**基础大模型构建了长文本的建模能力，使得模型具有语言生成能力，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含的构建了包括事实性知识（Factual Knowledge）和常识知识（Commonsense）在内的世界知识（World Knowledge）。**

> GPT-3 完成一次训练的总计算量是 3640PFlops，按照 NVIDIA A100 80G 和平均利用率达到 50% 计算，需要花费近一个月时间使用 1000 块 GPU 完成。

大规模语言模型的训练需要花费大量的计算资源和时间。包括 LLaMA 系列、Falcon 系列、百川（Baichuan）系列等在模型都属于此阶段。**由于训练过程需要消耗大量的计算资源，并很容易受到超参数影响，如何能够提升分布式计算效率并使得模型训练稳定收敛是本阶段的重点研究内容。**

### **有监督微调（Supervised Finetuning）**

该阶段也称为指令微调（Instruction Tuning），利用少量高质量数据集合，包含用户输入的提示词（Prompt）和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。

利用这些有监督数据，使用与预训练阶段**相同的语言模型训练算法**，在基础语言模型基础上再进行训练，从而得到有监督微调模型（SFT 模型）。经过训练的 SFT 模型具备了**初步的指令理解能力和上下文理解能力**，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。

由于有监督微调阶段的所需的训练语料数量较少，SFT 模型的训练过程并不需要消耗非常大量的计算。根据模型的大小和训练数据量，通常需要数十块 GPU，花费数天时间完成训练。SFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT的模型都属于该类型。]。当前的一些研究表明有监督微调阶段数据选择对 SFT 模型效果有非常大的影响，**因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点。**

### 奖励建模（Reward Modeling）

该阶段目标是构建一个文本质量对比模型，对于同一个提示词，SFT 模型给出的多个不同输出结果的质量进行排序。

奖励模型（RM 模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM 模型与基础语言模型和 SFT 模型不同，RM 模型本身并不能单独提供给用户使用。

<img src="..\..\img\llm-basic\reward-modeling.png" alt="img" style="zoom: 25%;" />

奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过几天时间完成训练。**由于 RM 模型的准确率对于强化学习阶段的效果有着至关重要的影响，因此对于该模型的训练通常需要大规模的训练数据。** Andrej Karpathy 在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要**花费非常长的时间才能完成**。标注其质量排序需要制定非常详细的规范，标注人员也需要非常认真的对标规范内容进行标注，需要消耗大量的人力，同时如何保持众包标注人员之间的一致性，也是奖励建模阶段需要解决的难点问题之一。

此外**奖励模型的泛化能力边界也在本阶段需要重点研究的另一个问题。如果 RM 模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定 RM 模型应用的泛化边界也是本阶段难点问题。**

### 强化学习（Reinforcement Learning）

利用 RM 输出的奖励，用强化学习方式微调优化 LM。长期以来出于工程和算法原因，人们认为用强化学习训练 LM 是不可能的。而目前多个组织找到的可行方案是使用策略梯度强化学习 (Policy Gradient RL) 算法、**近端策略优化 (Proximal Policy Optimization，PPO)** 微调初始 LM 的部分或全部参数。PPO 算法已经存在了相对较长的时间，有大量关于其原理的指南，因而成为 RLHF 中的有利选择。

[Link](https://huggingface.co/blog/zh/rlhf)

> 让我们首先将微调任务表述为 RL 问题。首先，该 **策略** (policy) 是一个接受提示并返回一系列文本 (或文本的概率分布) 的 语言模型。这个策略的 **行动空间** (action space) 是语言模型的词表对应的所有词元 (一般在 50k 数量级) ，**观察空间** (observation space) 是可能的输入词元序列，也比较大 (词汇量 ^ 输入标记的数量) 。**奖励函数** 是偏好模型和策略转变约束 (Policy shift constraint) 的结合。
>
> PPO 算法确定的奖励函数具体计算如下：将提示 $x$ 输入初始 LM 和当前微调的 LM，分别得到了输出文本$y1, y2$，将来自当前策略的文本传递给 RM 得到一个标量的奖励$r_\theta$。将两个模型的生成文本进行比较计算差异的惩罚项，在来自 OpenAI、Anthropic 和 DeepMind 的多篇论文中设计为输出词分布序列之间的 Kullback–Leibler [(KL) divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence) 散度的缩放，即 $r = r_\theta-\lambda r_{KL}$ 。这一项被用于惩罚 RL 策略在每个训练批次中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。此外，OpenAI 在 InstructGPT 上实验了在 PPO 添加新的预训练梯度，可以预见到奖励函数的公式会随着 RLHF 研究的进展而继续进化。
>
> 最后根据 PPO 算法，我们按当前批次数据的奖励指标进行优化 (来自 PPO 算法 on-policy 的特性) 。PPO 算法是一种信赖域优化 (Trust Region Optimization，TRO) 算法，它使用梯度约束确保更新步骤不会破坏学习过程的稳定性。DeepMind 对 Gopher 使用了类似的奖励设置，但是使用 A2C ([synchronous advantage actor-critic](http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com)) 算法来优化梯度



<img src="..\..\img\llm-basic\rlhf.png" alt="img" style="zoom:33%;" />



该阶段根据数十万用户给出的提示词，利用在前一阶段训练的 RM 模型，给出 SFT 模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。**该阶段所使用的提示词数量与有监督微调阶段类似，数量在十万量级，并且不需要人工提前给出该提示词所对应的理想回复。**使用强化学习，在 SFT 模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段所需要的计算量相较预训练阶段也少很多， 通常也仅需要数十块 GPU，经过数天时间的即可完成训练。

强化学习和有监督微调的对比，在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。此外，强化学习也并不是没有问题的，它会使得基础模型的熵降低，从而减少了模型输出的多样性。

在经过强化学习方法训练完成后的 RL 模型，就是最终提供给用户使用具有理解用户指令和上下文的类 ChatGPT 系统。**由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加 RM 模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难。**

