

> LLM 基于的 Transfomer 模型不再使用基于循环的方式建模文本输入，**序列中不再有任何信息能够提示模型单词之间的相对位置关系。**
> Transformer 模型在处理序列数据时，其**自注意力机制使得模型能够全局地捕捉不同元素之间的依赖关系**，但这样做的代价是丧失了序列中的元素顺序信息。由于自注意力机制并不考虑元素在序列中的位置，所以在输入序列的任何置换下都是不变的，这就意味着**模型无法区分序列中元素的相对位置**。在许多自然语言处理任务中，**词语之间的顺序是至关重要的**，所以需要一种方法来让模型捕获这一信息。
> 因此在送入编码器端建模其上下文语义之前，一个非常重要的操作是**在词嵌入中加入位置编码（Positional Encoding）**这一特征。
> 具体来说，**序列中每一个单词所在的位置都对应一个向量**。这一**向量会与单词表示对应相加**并送入到后续模块中做进一步处理。在训练的过程当中，**模型会自动地学习到如何利用这部分位置信息。**

常见的位置编码主要有绝对位置编码（sinusoidal），旋转位置编码（RoPE），以及相对位置编码ALiBi

## 绝对位置编码sinusoidal

绝对位置编码是直接将序列中每个位置的信息编码进模型的，从而使模型能够了解每个元素在序列中的具体位置。**原始Transformer提出时采用了sinusoidal位置编码，通过使用不同频率的正弦和余弦的函数，使得模型捕获位置之间的复杂关系，且这些编码与序列中每个位置的绝对值有关。**

sinusoidal位置编码公式如下：
$$
PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})
$$
其中，$pos$表示位置，$d_{model}$代表embedding的总维度，$2i, 2i+1$代表的是embedding不同位置的索引，即位置编码向量对应的维度位置。