## 1. LLM主流结构和训练目标

### 主流结构

目前LLM（Large Language Model）主流结构包括三种范式，分别为**Encoder-Decoder**、**Causal Decoder**、**Prefix Decoder**，如下图所示：

<img src="..\..\img\llm-basic\three-structures.png" alt="图片" style="zoom: 80%;" />

- Encoder-Decoder
  结构特点：输入双向注意力，输出单向注意力
  代表模型：T5、Flan-T5、BART
- Causal Decoder
  结构特点：从左到右的单向注意力
  代表模型：**LLaMA1/2系列、LLaMA衍生物**
- Prefix Decoder
  结构特点：输入双向注意力，输出单向注意力
  代表模型：ChatGLM、ChatGLM2、U-PaLM

### 结构对比

三种结构主要区别在于Attention Mask不同，如下图所示

<img src="..\..\img\llm-basic\three-masks.png" alt="图片" style="zoom: 67%;" />

- Encoder-Decoder
  特点：**在输入上采用双向注意力**，对问题的编码理解更充分;
  缺点：**在长文本生成任务上效果差，训练效率低**；
  适用任务：在偏理解的 NLP 任务上效果好。
- Causal Decoder
  特点：**自回归语言模型**，预训练和下游应用是完全一致的，**严格遵守只有后面的token才能看到前面的token的规则**；
  优点：训练效率高，zero-shot 能力更强，具有涌现能力；
  适用任务：文本生成任务效果好
- Prefix Decoder
  特点：**Prefix部分的token互相能看到**，属于Causal Decoder 和 Encoder-Decoder 的折中；
  缺点：训练效率低。

### 训练目标

#### 语言模型

根据已有词预测下一个词，即Next Token Prediction，是目前大模型所采用的最主流训练方式，训练目标为最大似然函数：
$$
L_{LM}(x) = \sum^n_{i=1}logP(x_i|x_{<i})
$$


训练效率：**Prefix Decoder < Causal Decoder**

Causal Decoder 结构会在**所有token上计算损失**，而Prefix Decoder只会在输出上计算损失。

#### 去噪自编码器

随机替换掉一些文本段，训练语言模型去恢复被打乱的文本段，即完形填空，训练目标函数为:
$$
L_{DAE}(x)=logP(\hat{x}|x_{/\hat{x}})
$$


去噪自编码器的实现难度更高，采用去噪自编码器作为训练目标的任务有GLM-130B、T5等。



## 2. LLM主流结构和训练目标

OpenAI 所使用的大规模语言模型构建流程如下图1所示。主要包含四个阶段：**预训练、有监督微调、奖励建模、强化学习**。这四个阶段都需要不同规模数据集合以及不同类型的算法，会产出不同类型的模 型，同时所需要的资源也有非常大的差别。

<img src="..\..\img\llm-basic\openai-flow.png" alt="图片" style="zoom:50%;" />

### 预训练（Pretraining）

该阶段需要利用海量的训练数据，包括互联网网页、维基百科、书籍、GitHub、 论文、问答网站等，构建包含数千亿甚至数万亿单词的具有多样性的内容。利用由数千块高性能 GPU 和高速网络组成超级计算机，花费数十天完成深度神经网络参数训练，构建基础语言模型 （Base Model）。

**基础大模型构建了长文本的建模能力，使得模型具有语言生成能力，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含的构建了包括事实性知识（Factual Knowledge）和常识知识（Commonsense）在内的世界知识（World Knowledge）。**

> GPT-3 完成一次训练的总计算量是 3640PFlops，按照 NVIDIA A100 80G 和平均利用率达到 50% 计算，需要花费近一个月时间使用 1000 块 GPU 完成。

大规模语言模型的训练需要花费大量的计算资源和时间。包括 LLaMA 系列、Falcon 系列、百川（Baichuan）系列等在模型都属于此阶段。**由于训练过程需要消耗大量的计算资源，并很容易受到超参数影响，如何能够提升分布式计算效率并使得模型训练稳定收敛是本阶段的重点研究内容。**

### **有监督微调（Supervised Finetuning）**

该阶段也称为指令微调（Instruction Tuning），利用少量高质量数据集合，包含用户输入的提示词（Prompt）和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。

利用这些有监督数据，使用与预训练阶段**相同的语言模型训练算法**，在基础语言模型基础上再进行训练，从而得到有监督微调模型（SFT 模型）。经过训练的 SFT 模型具备了**初步的指令理解能力和上下文理解能力**，能够完成开放领域问题、阅读理解、翻译、生成代码等能力，也具备了一定的对未知任务的泛化能力。

由于有监督微调阶段的所需的训练语料数量较少，SFT 模型的训练过程并不需要消耗非常大量的计算。根据模型的大小和训练数据量，通常需要数十块 GPU，花费数天时间完成训练。SFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT的模型都属于该类型。]。当前的一些研究表明有监督微调阶段数据选择对 SFT 模型效果有非常大的影响，**因此如何构造少量并且高质量的训练数据是本阶段有监督微调阶段的研究重点。**

### 奖励建模（Reward Modeling）

该阶段目标是构建一个文本质量对比模型，对于同一个提示词，SFT 模型给出的多个不同输出结果的质量进行排序。

奖励模型（RM 模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM 模型与基础语言模型和 SFT 模型不同，RM 模型本身并不能单独提供给用户使用。

<img src="..\..\img\llm-basic\reward-modeling.png" alt="img" style="zoom: 25%;" />

奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过几天时间完成训练。**由于 RM 模型的准确率对于强化学习阶段的效果有着至关重要的影响，因此对于该模型的训练通常需要大规模的训练数据。** Andrej Karpathy 在报告中指出，该部分需要百万量级的对比数据标注，而且其中很多标注需要**花费非常长的时间才能完成**。标注其质量排序需要制定非常详细的规范，标注人员也需要非常认真的对标规范内容进行标注，需要消耗大量的人力，同时如何保持众包标注人员之间的一致性，也是奖励建模阶段需要解决的难点问题之一。

此外**奖励模型的泛化能力边界也在本阶段需要重点研究的另一个问题。如果 RM 模型的目标是针对所有提示词系统所生成输出都能够高质量的进行判断，该问题所面临的难度在某种程度上与文本生成等价，因此如何限定 RM 模型应用的泛化边界也是本阶段难点问题。**

### 强化学习（Reinforcement Learning）

利用 RM 输出的奖励，用强化学习方式微调优化 LM

[Link](https://huggingface.co/blog/zh/rlhf)

该阶段根据数十万用户给出的提示词，利用在前一阶段训练的 RM 模型，给出 SFT 模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果。**该阶段所使用的提示词数量与有监督微调阶段类似，数量在十万量级，并且不需要人工提前给出该提示词所对应的理想回复。**使用强化学习，在 SFT 模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段所需要的计算量相较预训练阶段也少很多， 通常也仅需要数十块 GPU，经过数天时间的即可完成训练。

强化学习和有监督微调的对比，在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。此外，强化学习也并不是没有问题的，它会使得基础模型的熵降低，从而减少了模型输出的多样性。

在经过强化学习方法训练完成后的 RL 模型，就是最终提供给用户使用具有理解用户指令和上下文的类 ChatGPT 系统。**由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加 RM 模型的准确率问题，使得在大规模语言模型如何能够有效应用强化学习非常困难。**