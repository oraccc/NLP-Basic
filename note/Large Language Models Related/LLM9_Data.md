LLM 训练所需的数据来源大体上可以分为**通用数据**和**专业数据**两大类。

通用数据（General Data）包括网页、图书、新闻、对话文本等内容。通用数据具有规模大、多样性和易获取等特点，因此可以支持 LLM 的构建语言建模和泛化能力。

专业数据（Specialized Data）包括多语言数据、科学数据、代码以及领域特有资料等数据。通过在预训练阶段引入专业数据可以有效提供 LLM 的任务解决能力。

### 通用数据

通用数据在 LM 训练数据中占比通常非常高，主要包括网页、书籍、对话文本等类型，为大模型提供了大规模且多样的训练数据。

**网页（Webpage）是通用数据中数量最大的一类。**随着互联网的大规模普及，人们通过网站、论坛、博客、APP 等各种类型网站和应用，创造了海量的数据。因此，**如何过滤和处理网页以提升高质量数据的比例，对 LLM 训练来说非常重要。**

相较于其他语料库，**书籍也是最重要的，甚至是唯一的长文本书面语的数据来源。**书籍提供了完整的句子和段落，使得语言模型可以学习到上下文之间的联系。这对于模型理解句子中的复杂结构、逻辑关系和语义连贯性非常重要。书籍涵盖了各种文体和风格，包括小说、科学著作、历史记录等等。通过使用书籍数据训练 LM，可以使模型学习到不同的写作风格和表达方式，提高 LLM 在各种文本类型上的能力。

对话数据（Conversation Text）是指包含两个或更多参与者之间交流的文本内容。对话数据包含书面形式的对话、聊天记录、论坛帖子、社交媒体评论等。当前的一些研究也表明，对话数据可以有效增强 LM 的对话能力，并潜在地提高其在多种问答任务上的表现

### 专业数据

多语言数据（Multilingual Text）对于增强 LLM 语言理解和生成多语言能力具有至关重要的作用。

科学文本（Scientific Text）包括教材、论文、百科以及其他相关资源。这些数据对于提升 LLM 在理解科学知识方面具有重要作用

代码（Code）数据是进行程序生成任务所必须的训练数据。最近的研究和 ChatGPT 的结果表明，通过在大量代码上进行预训练，LLM 可以有效提升代码生成的效果。

### 数据处理

大语言模型的相关研究表明，数据质量对于模型的影响非常大。因此在收集到各类型数据之后，需要对数据进行处理，去除低质量数据、重复数据、有害信息、个人隐私等内容。典型的数据处理过程如下图所示，主要包含**质量过滤、冗余去除、隐私消除、词元切分**这四个步骤

#### 词元切分

传统的自然语言处理通常以单词为基本处理单元，模型都依赖预先确定的词表 **V**，在编码输入词序列时，这些词表示模型只能处理词表中存在的词。因此，在使用中，如果遇到不在词表中的未登录词，模型无法为其生成对应的表示，只能给予这些**未登录词（Out-of-vocabulary，OOV）**一个默认的通用表示。

在深度学习模型中，词表示模型会预先在词表中加入一个默认的“[UNK]”（unknown）标识，表示未知词，并在训练的过程中将 [UNK] 的向量作为词表示矩阵的一部分一起训练，通过 引入某些相应机制来更新 [UNK] 向量的参数。

在使用时，对于全部的未登录词，都使用 [UNK] 的向量作为这些词的表示向量。

此外，**基于固定词表的词表示模型对词表大小的选择比较敏感。当词表大小过小时，未登录词的比例较高，影响模型性能。而当词表大小过大时，大量低频词出现在词表中，而这些词的词向量很难得到充分学习。**

理想模式下，词表示模型应能覆盖绝大部分的输入词，并避免词表过大所造成的数据稀疏问题。

**为了缓解未登录词问题，一些工作通过利用亚词级别的信息构造词表示向量。**一种直接的解决思路是为输入建立字符级别表示，并通过字符向量的组合来获得每个单词的表示，以解决数据稀疏问题。然而，**单词中的词根、词缀等构词模式往往跨越多个字符，基于字符表示的方法很难学习跨度较大的模式。**为了充分学习这些构词模式，研究人员们提出了**子词词元化（Subword Tokenization）**方法，试图缓解上文介绍的未登录词问题。

**词元表示模型**会维护一个词元词表，其中既存在**完整的单词**，也存在形如“c”, “re”, “ing”等单词部分信息，称为**子词**。

**词元表示模型对词表中的每个词元计算一个定长向量表示**，供下游模型使用。对于输入的词序列，词元表示模型将每个词拆分为词表内的词元。例如，将单词“reborn”拆分为“re”和“born”。

模型随后查询每个词元的表示，将输入重新组成为词元表示序列。当下游模型需要计算一个单词或词组的表示时，可以将对应范围内的词元表示合成为需要的表示。因此，词元表示模型能够较好地解决自然语言处理系统中未登录词的问题。**词元分析（Tokenization）目标是将原始文本分割成由词元（Token）序列的过程。词元切分也是数据预处理中至关重要的一步。**

##### BPE

**字节对编码（Byte Pair Encoding，BPE）**模型是一种常见的子词词元模型。该模型所采用的词表包含最常见的单词以及高频出现的子词。在使用中，常见词通常本身位于 BPE 词表中，而罕见词通常能被分解为若干个包含在 BPE 词表中的词元，从而大幅度降低未登录词的比例。BPE 算法包括两个部分：（1）词元词表的确定；（2）全词切分为词元；（3）获得词元表示序列。计算过程下图所示。

<img src="..\..\img\llm-basic\BPE.png" alt="Image" style="zoom:67%;" />

在词元词表确定之后，对于输入词序列中未在词表中的全词进行切分，BPE 算法对词表中的词元按**从长到短**的顺序进行遍历，用每一个词元和当前序列中的全词或未完全切分为词元的部分进行匹配，将其切分为该词元和剩余部分的序列。

例如，对于单词“lowest\</w>”，首先通过匹配词元“est\</w>”将其切分为“low”, “est\</w>”的序列，再通过匹配词元“low”，确定其最终切分结果为“low”, “est\</w>”的序列。通过这样的过程，BPE 尽量将词序列中的词切分成已知的词元。

此外，字节级（Byte-level）BPE 通过将字节视为合并的基本符号，用来改善多语言语料库（例如包含非 ASCII 字符的文本）的分词质量。**GPT-2、BART 和 LLaMA** 等大语言模型都采用了这种分词方法。

原始 LLaMA 的词表大小是 32K，并且主要根据英文进行训练，因此，很多汉字都没有直接出现在词表中，需要字节来支持所有的中文字符，由 2 个或者 3 个 Byte Token 才能拼成一个完整的汉字。

###### BPE 输出

对于使用了字节对编码的大语言模型，其输出序列也是词元序列。对于原始输出，根据终结符 \</w> 的位置确定每个单词的范围，合并范围内的词元，将输出重新组合为词序列，作为最终的结果。

##### WordPiece 分词

WordPiece也是一种常见的词元分析算法，**最初应用于语音搜索系统**。此后，该算法**做为 BERT 的分词器**。

WordPiece 与 BPE 有非常相似的思想，都是通过迭代地合并连续的词元，但在合并的选择标准上略有不同。

为了进行合并，**WordPiece 需要首先训练一个语言模型**，并用该语言模型对所有可能的词元对进行评分。在每次合并时，选择使得训练数据**似然概率增加最多的词元对**。由于 Google 并没有发布其 WordPiece 算法的官方实现，HuggingFace 在其在线 NLP 课程中提供了一种更直观的选择度量方法：**一个词元对的评分是根据训练语料库中两个词元的共现计数除以它们各自的出现计数的乘积。**

##### Unigram 分词

Unigram 词元分析是另外一种应用于大语言模型的词元分析方法，T5 和 mBART 采用该方法构建词元分析器。不同于 BPE 和 WordPiece，Unigram 词元分析**从一个足够大的可能词元集合开始，然后迭代地从当前列表中删除词元，直到达到预期的词汇表大小为止。**

基于训练好的 Unigram 语言模型，使用从当前词汇表中删除某个字词后，训练语料库**似然性的增加量**作为选择标准。为了估计一元语言（Unigram）模型，采用了**期望最大化（Expectation–Maximization，EM）**算法：每次迭代中，首先根据旧的语言模型找到当前最佳的单词切分方式，然后重新估计一元语言单元概率以更新语言模型。

在这个过程中，使用**动态规划算法（如维特比算法）**来高效地找到给定语言模型时单词的最佳分解方式。

#### 冗余去除